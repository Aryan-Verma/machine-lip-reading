{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding3D\n",
    "from keras.layers.core import Lambda, Dropout, Flatten, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "DATA_PATH = CURRENT_PATH + '/../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    import tensorflow as tf\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # From Keras example image_ocr.py:\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    # y_pred = y_pred[:, 2:, :]\n",
    "    label_length = K.cast(tf.squeeze(label_length),'int32')\n",
    "    input_length = K.cast(tf.squeeze(input_length),'int32')\n",
    "    labels = K.ctc_label_dense_to_sparse(labels, label_length)\n",
    "    #y_pred = y_pred[:, :, :]\n",
    "    #return K.ctc_batch_cost(labels, y_pred, input_length, label_length, ignore_longer_outputs_than_inputs=True)\n",
    "    return tf.nn.ctc_loss(labels, y_pred, input_length, ctc_merge_repeated=False,\n",
    "                         ignore_longer_outputs_than_inputs = True, time_major = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTC(name, args):\n",
    "\treturn Lambda(ctc_lambda_func, output_shape=(1,), name=name)(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, output_size = 28, max_string_len = 10):\n",
    "    # model = Sequential()\n",
    "    input_data = Input(name='the_input', shape=input_size, dtype='float32')\n",
    "    x = ZeroPadding3D(padding=(0,2,2), name='padding1')(input_data)\n",
    "    x = TimeDistributed(Conv2D(filters = 32, kernel_size = 5, strides = (2,2),\n",
    "                             padding = 'same', activation = 'relu'))(x)\n",
    "    print\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(filters=32, kernel_size=5, strides=(2, 2),\n",
    "                               padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(filters=4, kernel_size=5, strides=(2, 2),\n",
    "                               padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    input_lstm = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    x_lstm = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(input_lstm)\n",
    "    x_lstm = Dense(output_size, kernel_initializer='he_normal', name='dense1')(x_lstm)\n",
    "    print(\"after dense1\")\n",
    "    y_pred = Activation('softmax', name='softmax')(x_lstm)\n",
    "\n",
    "    labels = Input(name='the_labels', shape = [max_string_len], dtype='int32')\n",
    "    input_length = Input(name = 'input_length', shape =[1], dtype = 'int32')\n",
    "    label_length = Input(name = 'label_length', shape = [1], dtype = 'int32')\n",
    "    loss = CTC('ctc',[y_pred, labels, input_length, label_length])\n",
    "    model = Model(inputs=[input_data, labels, label_length, input_length],\n",
    "                  outputs = loss)\n",
    "    model.summary()\n",
    "    # Build model here...\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_labels(labels, max_string_len):\n",
    "    padding = np.ones((labels.shape[0], max_string_len - labels.shape[1])) * -1\n",
    "    return np.concatenate((labels, padding), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, label_len_train, input_len_train, batch_size=256, epochs=100, val_train_ratio=0.2):\n",
    "    max_string_len = 10\n",
    "    if y_train.shape[1] != max_string_len:\n",
    "        y_train = pad_labels(y_train, max_string_len)\n",
    "\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)\n",
    "    history = model.fit(x = {'the_input':x_train, 'the_labels':y_train, 'label_length':label_len_train,\n",
    "                             'input_length':input_len_train}, y = {'ctc': np.zeros([x_train.shape[0]])},\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=val_train_ratio,\n",
    "                        shuffle=True,\n",
    "                        verbose=1)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    oh = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    x = list()\n",
    "    y = list()\n",
    "    t = list()\n",
    "    print(\"loading images...\")\n",
    "    for i, (img, words) in enumerate(load_data(DATA_PATH, verbose=False, framebyframe=False)):\n",
    "        if img.shape[0] != 75:\n",
    "            continue\n",
    "        x.append(img)\n",
    "        y.append(words)\n",
    "\n",
    "        t += words.tolist()\n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "    t = le.fit_transform(t)\n",
    "    oh.fit(t.reshape(-1, 1))\n",
    "\n",
    "    print(\"convering to np array...\")\n",
    "    x = np.stack(x, axis=0)\n",
    "\n",
    "    print(\"transforming y...\")\n",
    "    for i in range(len(y)):\n",
    "        y_ = le.transform(y[i])\n",
    "        y[i] = np.asarray(oh.transform(y_.reshape(-1, 1)).todense())\n",
    "    y = np.stack(y, axis=0)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from align import read_align\n",
    "from video import read_video\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import re\n",
    "\n",
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "DATA_PATH = CURRENT_PATH + '/../data'\n",
    "PREDICTOR_PATH = CURRENT_PATH + '/shape_predictor_68_face_landmarks.dat'\n",
    "SAVE_NUMPY_PATH = CURRENT_PATH + '/../data/numpy_results'\n",
    "\n",
    "\n",
    "def text_to_labels(text):\n",
    "    ret = []\n",
    "    for char in text:\n",
    "        if char >= 'a' and char <= 'z':\n",
    "            ret.append(ord(char) - ord('a'))\n",
    "        elif char == ' ':\n",
    "            ret.append(26)\n",
    "    return ret\n",
    "\n",
    "def labels_to_text(labels):\n",
    "# 26 is space, 27 is CTC blank char\n",
    "    text = ''\n",
    "    for c in labels:\n",
    "        if c >= 0 and c < 26:\n",
    "            text += chr(c + ord('a'))\n",
    "        elif c == 26:\n",
    "            text += ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from align import read_align\n",
    "from video import read_video\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "# CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "DATA_PATH = CURRENT_PATH + '/../data'\n",
    "PREDICTOR_PATH = CURRENT_PATH + '/shape_predictor_68_face_landmarks.dat'\n",
    "\n",
    "\n",
    "def text_to_labels(text):\n",
    "    ret = []\n",
    "    for char in text:\n",
    "        if char >= 'a' and char <= 'z':\n",
    "            ret.append(ord(char) - ord('a'))\n",
    "        elif char == ' ':\n",
    "            ret.append(26)\n",
    "    return ret\n",
    "\n",
    "def labels_to_text(labels):\n",
    "# 26 is space, 27 is CTC blank char\n",
    "    text = ''\n",
    "    for c in labels:\n",
    "        if c >= 0 and c < 26:\n",
    "            text += chr(c + ord('a'))\n",
    "        elif c == 26:\n",
    "            text += ' '\n",
    "    return text\n",
    "\n",
    "def load_data(datapath, speaker, verbose=True, num_samples=1000, ctc_encoding=True):\n",
    "    oh = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    counter = 0\n",
    "    done = False\n",
    "\n",
    "    max_len = 0\n",
    "    max_word_len = 0\n",
    "\n",
    "    x = list()\n",
    "    y = list()\n",
    "    \n",
    "    word_len_list = []\n",
    "    input_len_list = []\n",
    "    \n",
    "    path = datapath + '/' + str(speaker)\n",
    "    for root, dirs, files in os.walk(datapath):\n",
    "        for name in files:\n",
    "            if '.mpg' in name:\n",
    "                if verbose is True:\n",
    "                    print(str(counter) + \": reading - \" + root + name)\n",
    "\n",
    "                video = read_video(os.path.join(root, name), PREDICTOR_PATH)\n",
    "                alignments = read_align(os.path.join(root, '../align/', name.split(\".\")[0] + \".align\"))\n",
    "\n",
    "                for start, stop, word in alignments:\n",
    "                    if word == 'sil' or word == 'sp':\n",
    "                        continue\n",
    "                   \n",
    "                    if (len(x) > 0):\n",
    "                        _, d1, d2, d3 = video[start:stop].shape\n",
    "                        _, prev_d1, prev_d2, prev_d3 = x[-1].shape\n",
    "                        if (d1, d2, d3) != (prev_d1, prev_d2, prev_d3):\n",
    "                            if verbose is True:\n",
    "                                print(\"different size, skip\")\n",
    "                            continue\n",
    "                    \n",
    "                    x.append(video[start:stop])\n",
    "                    y.append(word)\n",
    "                            \n",
    "                    max_word_len = max(max_word_len, len(word))\n",
    "                    max_len = max(max_len, stop-start)\n",
    "\n",
    "                    word_len_list.append(len(word))\n",
    "                    input_len_list.append(stop-start)\n",
    "                    \n",
    "                    counter += 1\n",
    "                    if counter % num_samples == 0:\n",
    "                        \n",
    "                        if not ctc_encoding:\n",
    "                            y = le.fit_transform(y)\n",
    "                            y = oh.fit_transform(y.reshape(-1, 1)).todense()\n",
    "\n",
    "                        for i in range(len(x)):\n",
    "                            result = np.zeros((max_len, 50, 100, 3))\n",
    "                            result[:x[i].shape[0], :x[i].shape[1], :x[i].shape[2], :x[i].shape[3]] = x[i]\n",
    "                            x[i] = result\n",
    "\n",
    "                            if ctc_encoding:\n",
    "                                res = np.ones(max_word_len) * -1\n",
    "                                enc = np.array(text_to_labels(y[i]))\n",
    "                                res[:enc.shape[0]] = enc\n",
    "                                y[i] = res\n",
    "\n",
    "                        if ctc_encoding:\n",
    "                            y = np.stack(y, axis=0)\n",
    "\n",
    "                        x = np.stack(x, axis=0)\n",
    "\n",
    "                        print('saving numpy')\n",
    "                        np.savez_compressed(str(speaker) + '_x_' + str(counter % num_samples), x=x)\n",
    "                        np.savez_compressed(str(speaker) + '_y_' + str(counter % num_samples), y=y)\n",
    "                        np.savez_compressed(str(speaker) + '_wi_' + str(counter % num_samples),\n",
    "                                            word_length=word_len_list, input_length=input_len_list)\n",
    "                        \n",
    "\n",
    "                        max_len = 0\n",
    "                        max_word_len = 0\n",
    "\n",
    "                        x = list()\n",
    "                        y = list()\n",
    "\n",
    "                        word_len_list = []\n",
    "                        input_len_list = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videopwaj8p.mpg\n",
      "6: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videobbifzp.mpg\n",
      "12: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videobras7s.mpg\n",
      "18: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videosbwh8p.mpg\n",
      "24: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videolwwm1s.mpg\n",
      "30: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videobgbb2p.mpg\n",
      "36: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videolrik4p.mpg\n",
      "42: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videolgamzp.mpg\n",
      "48: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videopwwq8n.mpg\n",
      "54: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videolrar3a.mpg\n",
      "60: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videoprwx9a.mpg\n",
      "66: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videopgbe4p.mpg\n",
      "72: reading - /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/videolrwl6p.mpg\n"
     ]
    }
   ],
   "source": [
    "load_data(DATA_PATH, 's1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
