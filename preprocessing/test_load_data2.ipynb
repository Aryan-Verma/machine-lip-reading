{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding3D\n",
    "from keras.layers.core import Lambda, Dropout, Flatten, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "DATA_PATH = CURRENT_PATH + '/../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    import tensorflow as tf\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # From Keras example image_ocr.py:\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    # y_pred = y_pred[:, 2:, :]\n",
    "    label_length = K.cast(tf.squeeze(label_length),'int32')\n",
    "    input_length = K.cast(tf.squeeze(input_length),'int32')\n",
    "    labels = K.ctc_label_dense_to_sparse(labels, label_length)\n",
    "    #y_pred = y_pred[:, :, :]\n",
    "    #return K.ctc_batch_cost(labels, y_pred, input_length, label_length, ignore_longer_outputs_than_inputs=True)\n",
    "    return tf.nn.ctc_loss(labels, y_pred, input_length, ctc_merge_repeated=False,\n",
    "                         ignore_longer_outputs_than_inputs = True, time_major = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTC(name, args):\n",
    "\treturn Lambda(ctc_lambda_func, output_shape=(1,), name=name)(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, output_size = 28, max_string_len = 10):\n",
    "    # model = Sequential()\n",
    "    input_data = Input(name='the_input', shape=input_size, dtype='float32')\n",
    "    x = ZeroPadding3D(padding=(0,2,2), name='padding1')(input_data)\n",
    "    x = TimeDistributed(Conv2D(filters = 32, kernel_size = 5, strides = (2,2),\n",
    "                             padding = 'same', activation = 'relu'))(x)\n",
    "    print\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(filters=32, kernel_size=5, strides=(2, 2),\n",
    "                               padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(filters=4, kernel_size=5, strides=(2, 2),\n",
    "                               padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    input_lstm = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    x_lstm = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(input_lstm)\n",
    "    x_lstm = Dense(output_size, kernel_initializer='he_normal', name='dense1')(x_lstm)\n",
    "    print(\"after dense1\")\n",
    "    y_pred = Activation('softmax', name='softmax')(x_lstm)\n",
    "\n",
    "    labels = Input(name='the_labels', shape = [max_string_len], dtype='int32')\n",
    "    input_length = Input(name = 'input_length', shape =[1], dtype = 'int32')\n",
    "    label_length = Input(name = 'label_length', shape = [1], dtype = 'int32')\n",
    "    loss = CTC('ctc',[y_pred, labels, input_length, label_length])\n",
    "    model = Model(inputs=[input_data, labels, label_length, input_length],\n",
    "                  outputs = loss)\n",
    "    model.summary()\n",
    "    # Build model here...\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_labels(labels, max_string_len):\n",
    "    padding = np.ones((labels.shape[0], max_string_len - labels.shape[1])) * -1\n",
    "    return np.concatenate((labels, padding), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, label_len_train, input_len_train, batch_size=256, epochs=100, val_train_ratio=0.2):\n",
    "    max_string_len = 10\n",
    "    if y_train.shape[1] != max_string_len:\n",
    "        y_train = pad_labels(y_train, max_string_len)\n",
    "\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)\n",
    "    history = model.fit(x = {'the_input':x_train, 'the_labels':y_train, 'label_length':label_len_train,\n",
    "                             'input_length':input_len_train}, y = {'ctc': np.zeros([x_train.shape[0]])},\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=val_train_ratio,\n",
    "                        shuffle=True,\n",
    "                        verbose=1)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    oh = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    x = list()\n",
    "    y = list()\n",
    "    t = list()\n",
    "    print(\"loading images...\")\n",
    "    for i, (img, words) in enumerate(load_data(DATA_PATH, verbose=False, framebyframe=False)):\n",
    "        if img.shape[0] != 75:\n",
    "            continue\n",
    "        x.append(img)\n",
    "        y.append(words)\n",
    "\n",
    "        t += words.tolist()\n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "    t = le.fit_transform(t)\n",
    "    oh.fit(t.reshape(-1, 1))\n",
    "\n",
    "    print(\"convering to np array...\")\n",
    "    x = np.stack(x, axis=0)\n",
    "\n",
    "    print(\"transforming y...\")\n",
    "    for i in range(len(y)):\n",
    "        y_ = le.transform(y[i])\n",
    "        y[i] = np.asarray(oh.transform(y_.reshape(-1, 1)).todense())\n",
    "    y = np.stack(y, axis=0)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('load data took', 0.0897209644317627)\n",
      "('training data shapes:', (60, 16, 50, 100, 3), (60, 6))\n",
      "\n",
      "after dense1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 16, 50, 100,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "padding1 (ZeroPadding3D)        (None, 16, 54, 104,  0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 16, 27, 52, 3 2432        padding1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 16, 13, 26, 3 0           time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16, 13, 26, 3 0           time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 16, 7, 13, 32 25632       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 16, 3, 6, 32) 0           time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16, 3, 6, 32) 0           time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 16, 2, 3, 4)  3204        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 16, 1, 1, 4)  0           time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 16, 1, 1, 4)  0           time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 16, 4)        0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 16, 512)      400896      time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 16, 28)       14364       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 16, 28)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 446,528\n",
      "Trainable params: 446,528\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 38 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 18.3792 - val_loss: 18.0399\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 18.3637 - val_loss: 18.0278\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.3528 - val_loss: 18.0158\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 18.3502 - val_loss: 18.0023\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 18.3520 - val_loss: 17.9887\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.3198 - val_loss: 17.9741\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.2913 - val_loss: 17.9589\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.2986 - val_loss: 17.9439\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.2742 - val_loss: 17.9289\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 18.2811 - val_loss: 17.9132\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 18.2292 - val_loss: 17.8962\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.2180 - val_loss: 17.8791\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.1869 - val_loss: 17.8606\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.1966 - val_loss: 17.8397\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.1450 - val_loss: 17.8152\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 18.1259 - val_loss: 17.7903\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 18.0803 - val_loss: 17.7630\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 18.0678 - val_loss: 17.7344\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.0286 - val_loss: 17.7036\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.8932 - val_loss: 17.6630\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.8783 - val_loss: 17.6209\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 17.8233 - val_loss: 17.5694\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.7479 - val_loss: 17.5185\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.6839 - val_loss: 17.4628\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 17.6176 - val_loss: 17.3934\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 17.5952 - val_loss: 17.3114\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 17.4343 - val_loss: 17.2235\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 17.3321 - val_loss: 17.1312\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.2521 - val_loss: 17.0288\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.0923 - val_loss: 16.9163\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 17.0240 - val_loss: 16.7954\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.8996 - val_loss: 16.6631\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.7693 - val_loss: 16.5268\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.6558 - val_loss: 16.3887\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.5079 - val_loss: 16.2554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.4847 - val_loss: 16.1308\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.3456 - val_loss: 16.0167\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.2614 - val_loss: 15.9065\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.1120 - val_loss: 15.8066\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.0726 - val_loss: 15.7128\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.0161 - val_loss: 15.6256\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.0013 - val_loss: 15.5447\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7949 - val_loss: 15.4765\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7522 - val_loss: 15.4141\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7175 - val_loss: 15.3577\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6950 - val_loss: 15.3066\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.5944 - val_loss: 15.2623\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.5331 - val_loss: 15.2246\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.5691 - val_loss: 15.1907\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.5294 - val_loss: 15.1614\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.5076 - val_loss: 15.1365\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.5072 - val_loss: 15.1152\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 15.4894 - val_loss: 15.0966\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.4569 - val_loss: 15.0808\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.4226 - val_loss: 15.0668\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.4225 - val_loss: 15.0544\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.4128 - val_loss: 15.0434\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.3830 - val_loss: 15.0336\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.3602 - val_loss: 15.0247\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.3575 - val_loss: 15.0168\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.3334 - val_loss: 15.0097\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.3882 - val_loss: 15.0037\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.3113 - val_loss: 14.9983\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.3126 - val_loss: 14.9931\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.3139 - val_loss: 14.9884\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.3175 - val_loss: 14.9839\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.3103 - val_loss: 14.9797\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 15.2956 - val_loss: 14.9760\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 15.3180 - val_loss: 14.9726\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 15.2936 - val_loss: 14.9694\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2920 - val_loss: 14.9666\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.2996 - val_loss: 14.9639\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 15.3194 - val_loss: 14.9615\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2876 - val_loss: 14.9592\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2854 - val_loss: 14.9571\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.2931 - val_loss: 14.9552\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.3381 - val_loss: 14.9534\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2779 - val_loss: 14.9517\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2735 - val_loss: 14.9501\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.3195 - val_loss: 14.9486\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2897 - val_loss: 14.9471\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2809 - val_loss: 14.9458\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.2925 - val_loss: 14.9446\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2710 - val_loss: 14.9435\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2855 - val_loss: 14.9424\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2694 - val_loss: 14.9415\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.2900 - val_loss: 14.9406\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2695 - val_loss: 14.9398\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2976 - val_loss: 14.9390\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.3098 - val_loss: 14.9383\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2586 - val_loss: 14.9376\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2633 - val_loss: 14.9369\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.2990 - val_loss: 14.9363\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2661 - val_loss: 14.9356\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2556 - val_loss: 14.9350\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2595 - val_loss: 14.9344\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2866 - val_loss: 14.9338\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 15.2675 - val_loss: 14.9332\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2759 - val_loss: 14.9327\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.2549 - val_loss: 14.9321\n",
      "Saving model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from data import read_data_for_speaker\n",
    "speaker_id = 's13'\n",
    "if speaker_id:\n",
    "    start = time.time()\n",
    "    \n",
    "    x_raw, y_raw, word_len_list, input_len_list = read_data_for_speaker('s13')\n",
    "    end = time.time()\n",
    "    print(\"load data took\", end-start)\n",
    "    print(\"training data shapes:\", x_raw.shape, y_raw.shape)\n",
    "    x_train, x_test, y_train, y_test, label_len_train, label_len_test, \\\n",
    "    input_len_train, input_len_test = train_test_split(x_raw, y_raw, word_len_list, input_len_list, test_size=0.2)\n",
    "\n",
    "    model = build_model(x_raw.shape[1:], 28, max_string_len = 10)\n",
    "\n",
    "    history = train(model, x_train, y_train, label_len_train, input_len_train, epochs=100)\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    model.save('model_'+ speaker_id +\".h5\")\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new speaker added: s13\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae2p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq9s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaezn.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak4n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae1s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq8n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak7a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak5s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae3a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak6p.mpg\n",
      "new speaker added: s1\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbifzp.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbizzn.mpg\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbal8p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbbf9a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbas2p.mpg\n"
     ]
    }
   ],
   "source": [
    "SAVE_NUMPY_PATH = CURRENT_PATH + '/../data/numpy_results/'\n",
    "# Original load_data from data.py\n",
    "speakers = load_data(DATA_PATH, verbose=True, num_samples=80, ctc_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training data shapes:', (80, 16, 50, 100, 3), (80, 6))\n",
      "\n",
      "after dense1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 16, 50, 100,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "padding1 (ZeroPadding3D)        (None, 16, 54, 104,  0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 16, 27, 52, 3 2432        padding1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 16, 13, 26, 3 0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16, 13, 26, 3 0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 16, 7, 13, 32 25632       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 16, 3, 6, 32) 0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16, 3, 6, 32) 0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 16, 2, 3, 4)  3204        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 16, 1, 1, 4)  0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 1, 1, 4)  0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 16, 4)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 16, 512)      400896      time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 16, 28)       14364       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 16, 28)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 446,528\n",
      "Trainable params: 446,528\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 51 samples, validate on 13 samples\n",
      "Epoch 1/100\n",
      "51/51 [==============================] - 3s 60ms/step - loss: 17.3966 - val_loss: 20.8072\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 17.3603 - val_loss: 20.7422\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 17.3265 - val_loss: 20.6468\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 17.2922 - val_loss: 20.5430\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 17.2723 - val_loss: 20.4447\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 17.2098 - val_loss: 20.3495\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 17.1532 - val_loss: 20.2449\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 17.1066 - val_loss: 20.1408\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 17.0475 - val_loss: 20.0321\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.9993 - val_loss: 19.9114\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 16.9117 - val_loss: 19.7877\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.8455 - val_loss: 19.6568\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.7768 - val_loss: 19.5180\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.6837 - val_loss: 19.3800\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.5874 - val_loss: 19.2379\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.4491 - val_loss: 19.0936\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 16.3826 - val_loss: 18.9510\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 16.2972 - val_loss: 18.8136\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.1800 - val_loss: 18.6814\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.0716 - val_loss: 18.5548\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.9849 - val_loss: 18.4426\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.9408 - val_loss: 18.3326\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 15.8090 - val_loss: 18.2412\n",
      "Epoch 24/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.7543 - val_loss: 18.1563\n",
      "Epoch 25/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.6522 - val_loss: 18.0726\n",
      "Epoch 26/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.6060 - val_loss: 17.9959\n",
      "Epoch 27/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.5429 - val_loss: 17.9210\n",
      "Epoch 28/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.5086 - val_loss: 17.8522\n",
      "Epoch 29/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.4189 - val_loss: 17.7920\n",
      "Epoch 30/100\n",
      "51/51 [==============================] - 2s 40ms/step - loss: 15.3162 - val_loss: 17.7371\n",
      "Epoch 31/100\n",
      "51/51 [==============================] - 2s 47ms/step - loss: 15.3425 - val_loss: 17.6871\n",
      "Epoch 32/100\n",
      "51/51 [==============================] - 2s 41ms/step - loss: 15.1840 - val_loss: 17.6396\n",
      "Epoch 33/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.2175 - val_loss: 17.5954\n",
      "Epoch 34/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.1550 - val_loss: 17.5560\n",
      "Epoch 35/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.1742 - val_loss: 17.5191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.1430 - val_loss: 17.4848\n",
      "Epoch 37/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.1448 - val_loss: 17.4540\n",
      "Epoch 38/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.1063 - val_loss: 17.4264\n",
      "Epoch 39/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.0708 - val_loss: 17.4018\n",
      "Epoch 40/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.0303 - val_loss: 17.3798\n",
      "Epoch 41/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.0064 - val_loss: 17.3610\n",
      "Epoch 42/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9974 - val_loss: 17.3454\n",
      "Epoch 43/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9695 - val_loss: 17.3325\n",
      "Epoch 44/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9716 - val_loss: 17.3217\n",
      "Epoch 45/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9547 - val_loss: 17.3124\n",
      "Epoch 46/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.9544 - val_loss: 17.3043\n",
      "Epoch 47/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9426 - val_loss: 17.2974\n",
      "Epoch 48/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9453 - val_loss: 17.2913\n",
      "Epoch 49/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9412 - val_loss: 17.2858\n",
      "Epoch 50/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.9368 - val_loss: 17.2809\n",
      "Epoch 51/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9424 - val_loss: 17.2764\n",
      "Epoch 52/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9270 - val_loss: 17.2724\n",
      "Epoch 53/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.9193 - val_loss: 17.2688\n",
      "Epoch 54/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9173 - val_loss: 17.2656\n",
      "Epoch 55/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9208 - val_loss: 17.2627\n",
      "Epoch 56/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9302 - val_loss: 17.2600\n",
      "Epoch 57/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9072 - val_loss: 17.2575\n",
      "Epoch 58/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9154 - val_loss: 17.2554\n",
      "Epoch 59/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8990 - val_loss: 17.2534\n",
      "Epoch 60/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9016 - val_loss: 17.2516\n",
      "Epoch 61/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9120 - val_loss: 17.2499\n",
      "Epoch 62/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9003 - val_loss: 17.2483\n",
      "Epoch 63/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8988 - val_loss: 17.2468\n",
      "Epoch 64/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8893 - val_loss: 17.2453\n",
      "Epoch 65/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8947 - val_loss: 17.2440\n",
      "Epoch 66/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8984 - val_loss: 17.2428\n",
      "Epoch 67/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9033 - val_loss: 17.2417\n",
      "Epoch 68/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8881 - val_loss: 17.2405\n",
      "Epoch 69/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8798 - val_loss: 17.2395\n",
      "Epoch 70/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8847 - val_loss: 17.2384\n",
      "Epoch 71/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8909 - val_loss: 17.2373\n",
      "Epoch 72/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8834 - val_loss: 17.2362\n",
      "Epoch 73/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8953 - val_loss: 17.2351\n",
      "Epoch 74/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8848 - val_loss: 17.2340\n",
      "Epoch 75/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8729 - val_loss: 17.2330\n",
      "Epoch 76/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8816 - val_loss: 17.2321\n",
      "Epoch 77/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8820 - val_loss: 17.2312\n",
      "Epoch 78/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8832 - val_loss: 17.2303\n",
      "Epoch 79/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8758 - val_loss: 17.2295\n",
      "Epoch 80/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8852 - val_loss: 17.2287\n",
      "Epoch 81/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8762 - val_loss: 17.2279\n",
      "Epoch 82/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8747 - val_loss: 17.2271\n",
      "Epoch 83/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8677 - val_loss: 17.2264\n",
      "Epoch 84/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8812 - val_loss: 17.2257\n",
      "Epoch 85/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8673 - val_loss: 17.2251\n",
      "Epoch 86/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8825 - val_loss: 17.2245\n",
      "Epoch 87/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8883 - val_loss: 17.2240\n",
      "Epoch 88/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8789 - val_loss: 17.2234\n",
      "Epoch 89/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8720 - val_loss: 17.2229\n",
      "Epoch 90/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8742 - val_loss: 17.2224\n",
      "Epoch 91/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8733 - val_loss: 17.2219\n",
      "Epoch 92/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8744 - val_loss: 17.2215\n",
      "Epoch 93/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8651 - val_loss: 17.2211\n",
      "Epoch 94/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8705 - val_loss: 17.2207\n",
      "Epoch 95/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8746 - val_loss: 17.2203\n",
      "Epoch 96/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8746 - val_loss: 17.2200\n",
      "Epoch 97/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8715 - val_loss: 17.2196\n",
      "Epoch 98/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8713 - val_loss: 17.2193\n",
      "Epoch 99/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8638 - val_loss: 17.2190\n",
      "Epoch 100/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8673 - val_loss: 17.2187\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "x, y, label_len, input_len = speakers\n",
    "print(\"training data shapes:\", x.shape, y.shape)\n",
    "x_train, x_test, y_train, y_test, label_len_train, label_len_test, \\\n",
    "input_len_train, input_len_test = train_test_split(x, y, label_len, input_len, test_size=0.2)\n",
    "\n",
    "model = build_model(x.shape[1:], 28, max_string_len = 10)\n",
    "\n",
    "history = train(model, x_train, y_train, label_len_train, input_len_train, epochs=epochs)\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_data2 takes the same inputs as load_data, but detect speakers and save corresponding numpy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from align import read_align\n",
    "from video import read_video\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import re\n",
    "\n",
    "# CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "DATA_PATH = CURRENT_PATH + '/../data'\n",
    "PREDICTOR_PATH = CURRENT_PATH + '/shape_predictor_68_face_landmarks.dat'\n",
    "SAVE_NUMPY_PATH = CURRENT_PATH + '/../data/numpy_results'\n",
    "\n",
    "\n",
    "def text_to_labels(text):\n",
    "    ret = []\n",
    "    for char in text:\n",
    "        if char >= 'a' and char <= 'z':\n",
    "            ret.append(ord(char) - ord('a'))\n",
    "        elif char == ' ':\n",
    "            ret.append(26)\n",
    "    return ret\n",
    "\n",
    "def labels_to_text(labels):\n",
    "# 26 is space, 27 is CTC blank char\n",
    "    text = ''\n",
    "    for c in labels:\n",
    "        if c >= 0 and c < 26:\n",
    "            text += chr(c + ord('a'))\n",
    "        elif c == 26:\n",
    "            text += ' '\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_data2(datapath, verbose=False, num_samples=-1, ctc_encoding=False):\n",
    "    oh = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    counter = 0\n",
    "    done = False\n",
    "\n",
    "    max_len = 0\n",
    "    max_word_len = 0\n",
    "\n",
    "    word_len_list = []\n",
    "    input_len_list = []\n",
    "\n",
    "    x_raw = list()\n",
    "    y_raw = list()\n",
    "    \n",
    "    pattern = re.compile(\"s[0-9]\") \n",
    "    speakers = []\n",
    "    x_raws = {}\n",
    "    y_raws = {}\n",
    "    word_len_lists = {}\n",
    "    input_len_lists = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(datapath):\n",
    "        check = root.split(\"/\")[-1]\n",
    "        match = pattern.findall(check)\n",
    "        if (len(match) > 0):\n",
    "            if check.index(match[0]) == 0 and check not in speakers:             \n",
    "                speakers.append(check)\n",
    "                x_raws[check] = []\n",
    "                y_raws[check] = []\n",
    "                word_len_lists[check] = []\n",
    "                input_len_lists[check] = []\n",
    "                if verbose:\n",
    "                    print(\"new speaker added: \" + check)            \n",
    "        \n",
    "        for name in files:\n",
    "            if '.mpg' in name:\n",
    "                if verbose is True:\n",
    "                    print(\"reading: \" + root + \"/\" + name)\n",
    "\n",
    "                video = read_video(os.path.join(root, name), PREDICTOR_PATH)\n",
    "                alignments = read_align(os.path.join(root, '../align/', name.split(\".\")[0] + \".align\"))\n",
    "                \n",
    "                for start, stop, word in alignments:\n",
    "                    if word == 'sil' or word == 'sp':\n",
    "                        continue\n",
    "                    \n",
    "#                     if verbose is True:\n",
    "#                         print(str(counter) + \": \" + str(start) + \"--\" + str(stop) + \": \" + word)\n",
    "                    \n",
    "                    _, d1, d2, d3 = video[start:stop].shape\n",
    "                    \n",
    "                    if (len(x_raw) > 0):\n",
    "                        _, prev_d1, prev_d2, prev_d3 = x_raw[-1].shape\n",
    "                        if (d1, d2, d3) != (prev_d1, prev_d2, prev_d3):\n",
    "                            if verbose is True:\n",
    "                                print(\"different size, skip\")\n",
    "                            continue\n",
    "                    \n",
    "                    x_raw.append(video[start:stop])\n",
    "                    y_raw.append(word)\n",
    "                    \n",
    "                    x_raws[speakers[-1]].append(video[start:stop])\n",
    "                    y_raws[speakers[-1]].append(word)\n",
    "                    \n",
    "                    max_word_len = max(max_word_len, len(word))\n",
    "                    max_len = max(max_len, stop-start)\n",
    "                    word_len_list.append(len(word))\n",
    "                    input_len_list.append(stop-start)\n",
    "                    \n",
    "                    word_len_lists[speakers[-1]].append(len(word))\n",
    "                    input_len_lists[speakers[-1]].append(stop-start)\n",
    "                    \n",
    "                    counter += 1\n",
    "                    if counter == num_samples:\n",
    "                        done = True\n",
    "                        break\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if not ctc_encoding:\n",
    "        y_raw = le.fit_transform(y_raw)\n",
    "        y = oh.fit_transform(y_raw.reshape(-1, 1)).todense()\n",
    "\n",
    "\n",
    "    for i in range(len(x_raw)):\n",
    "        result = np.zeros((max_len, x_raw[i].shape[1], x_raw[i].shape[2], x_raw[i].shape[3]))\n",
    "        result[:x_raw[i].shape[0], :x_raw[i].shape[1], :x_raw[i].shape[2], :x_raw[i].shape[3]] = x_raw[i]\n",
    "        x_raw[i] = result\n",
    "        \n",
    "        \n",
    "        if ctc_encoding:\n",
    "            res = np.ones(max_word_len) * -1\n",
    "            enc = np.array(text_to_labels(y_raw[i]))\n",
    "            res[:enc.shape[0]] = enc\n",
    "            y_raw[i] = res\n",
    "\n",
    "    \n",
    "    \n",
    "    for speaker in speakers:\n",
    "        \n",
    "        for i in range(len(x_raws[speaker])):\n",
    "            result = np.zeros((max_len, x_raws[speaker][i].shape[1], x_raws[speaker][i].shape[2], x_raws[speaker][i].shape[3]))\n",
    "            result[:x_raws[speaker][i].shape[0], :x_raws[speaker][i].shape[1], :x_raws[speaker][i].shape[2], :x_raws[speaker][i].shape[3]] = x_raws[speaker][i]\n",
    "            x_raws[speaker][i] = result\n",
    "\n",
    "\n",
    "            if ctc_encoding:\n",
    "                res = np.ones(max_word_len) * -1\n",
    "                enc = np.array(text_to_labels(y_raws[speaker][i]))\n",
    "                res[:enc.shape[0]] = enc\n",
    "                y_raws[speaker][i] = res\n",
    "\n",
    "        np_save = SAVE_NUMPY_PATH + \"/\" + speaker\n",
    "#         x_raws[speaker] = np.stack(x_raws[speaker], axis=0)\n",
    "        np.save(np_save + \"_x2\", x_raws[speaker]) \n",
    "#         if ctc_encoding:\n",
    "#             y = np.stack(y_raw, axis=0)\n",
    "        np.save(np_save + \"_y2\", y_raws[speaker])\n",
    "        np.save(np_save + \"_word_len_list2\", np.array(word_len_lists[speaker]))\n",
    "        np.save(np_save + \"_input_len_list2\", np.array(input_len_lists[speaker]))\n",
    "        \n",
    "    if ctc_encoding:\n",
    "        y = np.stack(y_raw, axis=0)\n",
    "\n",
    "    x = np.stack(x_raw, axis=0)\n",
    "    return speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new speaker added: s13\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae2p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq9s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaezn.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak4n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae1s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq8n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak7a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak5s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae3a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak6p.mpg\n",
      "new speaker added: s1\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbifzp.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbizzn.mpg\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbal8p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbbf9a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbas2p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbaf5a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbir6n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbil5a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbaf3s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbiz3a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbbm1s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbas1s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbbz9s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbif1a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbaz5s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbbs6p.mpg\n"
     ]
    }
   ],
   "source": [
    "speakers = load_data2(DATA_PATH, verbose=True, num_samples=150, ctc_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey let's loading the numpy data for speaker s13\n",
      "reading x_raw: \n",
      "(60, 16, 50, 100, 3)\n",
      "reading y_raw: \n",
      "(60, 6)\n",
      "reading word_len_list: \n",
      "(60,)\n",
      "reading input_len_list: \n",
      "(60,)\n",
      "('training data shapes:', (60, 16, 50, 100, 3), (60, 6))\n",
      "\n",
      "after dense1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 16, 50, 100,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "padding1 (ZeroPadding3D)        (None, 16, 54, 104,  0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 16, 27, 52, 3 2432        padding1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, 16, 13, 26, 3 0           time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 16, 13, 26, 3 0           time_distributed_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistri (None, 16, 7, 13, 32 25632       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistri (None, 16, 3, 6, 32) 0           time_distributed_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 16, 3, 6, 32) 0           time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (None, 16, 2, 3, 4)  3204        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistri (None, 16, 1, 1, 4)  0           time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16, 1, 1, 4)  0           time_distributed_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_21 (TimeDistri (None, 16, 4)        0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 16, 512)      400896      time_distributed_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 16, 28)       14364       bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 16, 28)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 446,528\n",
      "Trainable params: 446,528\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 38 samples, validate on 10 samples\n",
      "Epoch 1/200\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 18.9370 - val_loss: 20.9327\n",
      "Epoch 2/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.8865 - val_loss: 20.8881\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.8783 - val_loss: 20.8339\n",
      "Epoch 4/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.8567 - val_loss: 20.7825\n",
      "Epoch 5/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.8676 - val_loss: 20.7360\n",
      "Epoch 6/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.7966 - val_loss: 20.6859\n",
      "Epoch 7/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.7783 - val_loss: 20.6267\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.7252 - val_loss: 20.5609\n",
      "Epoch 9/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.6803 - val_loss: 20.4861\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.6309 - val_loss: 20.3826\n",
      "Epoch 11/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.5677 - val_loss: 20.2593\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.5142 - val_loss: 20.1288\n",
      "Epoch 13/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.4701 - val_loss: 19.9691\n",
      "Epoch 14/200\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 18.4686 - val_loss: 19.8168\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.3349 - val_loss: 19.6695\n",
      "Epoch 16/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.2526 - val_loss: 19.5327\n",
      "Epoch 17/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.2744 - val_loss: 19.4070\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.1357 - val_loss: 19.2906\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.9389 - val_loss: 19.1809\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.8644 - val_loss: 19.0694\n",
      "Epoch 21/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.8743 - val_loss: 18.9578\n",
      "Epoch 22/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.6859 - val_loss: 18.8538\n",
      "Epoch 23/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 17.5888 - val_loss: 18.7524\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 17.4503 - val_loss: 18.6624\n",
      "Epoch 25/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.3920 - val_loss: 18.5753\n",
      "Epoch 26/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 17.2852 - val_loss: 18.4949\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.0921 - val_loss: 18.4238\n",
      "Epoch 28/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 17.1225 - val_loss: 18.3564\n",
      "Epoch 29/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.9324 - val_loss: 18.2895\n",
      "Epoch 30/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.7927 - val_loss: 18.2257\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.7076 - val_loss: 18.1671\n",
      "Epoch 32/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.6405 - val_loss: 18.1121\n",
      "Epoch 33/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.5768 - val_loss: 18.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.4708 - val_loss: 18.0158\n",
      "Epoch 35/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.4081 - val_loss: 17.9732\n",
      "Epoch 36/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.4147 - val_loss: 17.9359\n",
      "Epoch 37/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.2970 - val_loss: 17.9029\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.2560 - val_loss: 17.8741\n",
      "Epoch 39/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.2529 - val_loss: 17.8475\n",
      "Epoch 40/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.1346 - val_loss: 17.8246\n",
      "Epoch 41/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.1186 - val_loss: 17.8044\n",
      "Epoch 42/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.0225 - val_loss: 17.7863\n",
      "Epoch 43/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0528 - val_loss: 17.7707\n",
      "Epoch 44/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0408 - val_loss: 17.7569\n",
      "Epoch 45/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0186 - val_loss: 17.7447\n",
      "Epoch 46/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.9366 - val_loss: 17.7336\n",
      "Epoch 47/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.8934 - val_loss: 17.7235\n",
      "Epoch 48/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9270 - val_loss: 17.7145\n",
      "Epoch 49/200\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 15.8549 - val_loss: 17.7064\n",
      "Epoch 50/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.8560 - val_loss: 17.6992\n",
      "Epoch 51/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.8168 - val_loss: 17.6926\n",
      "Epoch 52/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.8311 - val_loss: 17.6869\n",
      "Epoch 53/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.8450 - val_loss: 17.6816\n",
      "Epoch 54/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.8359 - val_loss: 17.6768\n",
      "Epoch 55/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.8341 - val_loss: 17.6725\n",
      "Epoch 56/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.8128 - val_loss: 17.6686\n",
      "Epoch 57/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7929 - val_loss: 17.6651\n",
      "Epoch 58/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7871 - val_loss: 17.6619\n",
      "Epoch 59/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7840 - val_loss: 17.6590\n",
      "Epoch 60/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7755 - val_loss: 17.6564\n",
      "Epoch 61/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7789 - val_loss: 17.6541\n",
      "Epoch 62/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7695 - val_loss: 17.6520\n",
      "Epoch 63/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7839 - val_loss: 17.6501\n",
      "Epoch 64/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7589 - val_loss: 17.6483\n",
      "Epoch 65/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7538 - val_loss: 17.6466\n",
      "Epoch 66/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7603 - val_loss: 17.6451\n",
      "Epoch 67/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7457 - val_loss: 17.6436\n",
      "Epoch 68/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7470 - val_loss: 17.6423\n",
      "Epoch 69/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7370 - val_loss: 17.6411\n",
      "Epoch 70/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7504 - val_loss: 17.6399\n",
      "Epoch 71/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7504 - val_loss: 17.6388\n",
      "Epoch 72/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7316 - val_loss: 17.6378\n",
      "Epoch 73/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7330 - val_loss: 17.6369\n",
      "Epoch 74/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7294 - val_loss: 17.6359\n",
      "Epoch 75/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7313 - val_loss: 17.6351\n",
      "Epoch 76/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7221 - val_loss: 17.6343\n",
      "Epoch 77/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7262 - val_loss: 17.6335\n",
      "Epoch 78/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7366 - val_loss: 17.6328\n",
      "Epoch 79/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7235 - val_loss: 17.6321\n",
      "Epoch 80/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7212 - val_loss: 17.6314\n",
      "Epoch 81/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7157 - val_loss: 17.6308\n",
      "Epoch 82/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7255 - val_loss: 17.6302\n",
      "Epoch 83/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7145 - val_loss: 17.6296\n",
      "Epoch 84/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7231 - val_loss: 17.6291\n",
      "Epoch 85/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7159 - val_loss: 17.6286\n",
      "Epoch 86/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7123 - val_loss: 17.6281\n",
      "Epoch 87/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7217 - val_loss: 17.6276\n",
      "Epoch 88/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7130 - val_loss: 17.6272\n",
      "Epoch 89/200\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 15.7259 - val_loss: 17.6267\n",
      "Epoch 90/200\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 15.7260 - val_loss: 17.6263\n",
      "Epoch 91/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7323 - val_loss: 17.6259\n",
      "Epoch 92/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7174 - val_loss: 17.6255\n",
      "Epoch 93/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7072 - val_loss: 17.6252\n",
      "Epoch 94/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7067 - val_loss: 17.6248\n",
      "Epoch 95/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7181 - val_loss: 17.6245\n",
      "Epoch 96/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7106 - val_loss: 17.6241\n",
      "Epoch 97/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7099 - val_loss: 17.6238\n",
      "Epoch 98/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7111 - val_loss: 17.6235\n",
      "Epoch 99/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7071 - val_loss: 17.6232\n",
      "Epoch 100/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7069 - val_loss: 17.6229\n",
      "Epoch 101/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7103 - val_loss: 17.6226\n",
      "Epoch 102/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7176 - val_loss: 17.6223\n",
      "Epoch 103/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6994 - val_loss: 17.6221\n",
      "Epoch 104/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7026 - val_loss: 17.6218\n",
      "Epoch 105/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7026 - val_loss: 17.6215\n",
      "Epoch 106/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7036 - val_loss: 17.6213\n",
      "Epoch 107/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7090 - val_loss: 17.6210\n",
      "Epoch 108/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7011 - val_loss: 17.6208\n",
      "Epoch 109/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7066 - val_loss: 17.6205\n",
      "Epoch 110/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7154 - val_loss: 17.6203\n",
      "Epoch 111/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7070 - val_loss: 17.6201\n",
      "Epoch 112/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7082 - val_loss: 17.6199\n",
      "Epoch 113/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7070 - val_loss: 17.6196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7018 - val_loss: 17.6194\n",
      "Epoch 115/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7011 - val_loss: 17.6192\n",
      "Epoch 116/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7014 - val_loss: 17.6190\n",
      "Epoch 117/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6957 - val_loss: 17.6188\n",
      "Epoch 118/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7003 - val_loss: 17.6186\n",
      "Epoch 119/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7029 - val_loss: 17.6184\n",
      "Epoch 120/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7113 - val_loss: 17.6182\n",
      "Epoch 121/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7104 - val_loss: 17.6180\n",
      "Epoch 122/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6995 - val_loss: 17.6178\n",
      "Epoch 123/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7052 - val_loss: 17.6176\n",
      "Epoch 124/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6984 - val_loss: 17.6174\n",
      "Epoch 125/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7004 - val_loss: 17.6172\n",
      "Epoch 126/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7015 - val_loss: 17.6171\n",
      "Epoch 127/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7150 - val_loss: 17.6169\n",
      "Epoch 128/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7066 - val_loss: 17.6168\n",
      "Epoch 129/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6991 - val_loss: 17.6166\n",
      "Epoch 130/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6954 - val_loss: 17.6164\n",
      "Epoch 131/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6937 - val_loss: 17.6163\n",
      "Epoch 132/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6987 - val_loss: 17.6162\n",
      "Epoch 133/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6952 - val_loss: 17.6160\n",
      "Epoch 134/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6932 - val_loss: 17.6159\n",
      "Epoch 135/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6962 - val_loss: 17.6158\n",
      "Epoch 136/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6930 - val_loss: 17.6156\n",
      "Epoch 137/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6960 - val_loss: 17.6155\n",
      "Epoch 138/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6953 - val_loss: 17.6154\n",
      "Epoch 139/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6926 - val_loss: 17.6152\n",
      "Epoch 140/200\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 15.6929 - val_loss: 17.6151\n",
      "Epoch 141/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6912 - val_loss: 17.6150\n",
      "Epoch 142/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6954 - val_loss: 17.6149\n",
      "Epoch 143/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.7050 - val_loss: 17.6147\n",
      "Epoch 144/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7005 - val_loss: 17.6146\n",
      "Epoch 145/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6913 - val_loss: 17.6145\n",
      "Epoch 146/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6927 - val_loss: 17.6143\n",
      "Epoch 147/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.7019 - val_loss: 17.6142\n",
      "Epoch 148/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6936 - val_loss: 17.6141\n",
      "Epoch 149/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6910 - val_loss: 17.6139\n",
      "Epoch 150/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6927 - val_loss: 17.6138\n",
      "Epoch 151/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6946 - val_loss: 17.6137\n",
      "Epoch 152/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6891 - val_loss: 17.6136\n",
      "Epoch 153/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6892 - val_loss: 17.6135\n",
      "Epoch 154/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6949 - val_loss: 17.6133\n",
      "Epoch 155/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6988 - val_loss: 17.6132\n",
      "Epoch 156/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6913 - val_loss: 17.6131\n",
      "Epoch 157/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6898 - val_loss: 17.6130\n",
      "Epoch 158/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6886 - val_loss: 17.6129\n",
      "Epoch 159/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6942 - val_loss: 17.6128\n",
      "Epoch 160/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6928 - val_loss: 17.6127\n",
      "Epoch 161/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6913 - val_loss: 17.6126\n",
      "Epoch 162/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6887 - val_loss: 17.6125\n",
      "Epoch 163/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6937 - val_loss: 17.6124\n",
      "Epoch 164/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6941 - val_loss: 17.6123\n",
      "Epoch 165/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6931 - val_loss: 17.6122\n",
      "Epoch 166/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6937 - val_loss: 17.6121\n",
      "Epoch 167/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6879 - val_loss: 17.6120\n",
      "Epoch 168/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6911 - val_loss: 17.6119\n",
      "Epoch 169/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6890 - val_loss: 17.6118\n",
      "Epoch 170/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6938 - val_loss: 17.6117\n",
      "Epoch 171/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6900 - val_loss: 17.6116\n",
      "Epoch 172/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6894 - val_loss: 17.6115\n",
      "Epoch 173/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6887 - val_loss: 17.6114\n",
      "Epoch 174/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6881 - val_loss: 17.6113\n",
      "Epoch 175/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6858 - val_loss: 17.6113\n",
      "Epoch 176/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6887 - val_loss: 17.6112\n",
      "Epoch 177/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6886 - val_loss: 17.6111\n",
      "Epoch 178/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6886 - val_loss: 17.6110\n",
      "Epoch 179/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6896 - val_loss: 17.6110\n",
      "Epoch 180/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6890 - val_loss: 17.6109\n",
      "Epoch 181/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6920 - val_loss: 17.6108\n",
      "Epoch 182/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6919 - val_loss: 17.6108\n",
      "Epoch 183/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6904 - val_loss: 17.6107\n",
      "Epoch 184/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6885 - val_loss: 17.6106\n",
      "Epoch 185/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6861 - val_loss: 17.6105\n",
      "Epoch 186/200\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 15.6874 - val_loss: 17.6105\n",
      "Epoch 187/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6908 - val_loss: 17.6104\n",
      "Epoch 188/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6919 - val_loss: 17.6103\n",
      "Epoch 189/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6863 - val_loss: 17.6102\n",
      "Epoch 190/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6856 - val_loss: 17.6102\n",
      "Epoch 191/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6954 - val_loss: 17.6101\n",
      "Epoch 192/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6868 - val_loss: 17.6100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6912 - val_loss: 17.6100\n",
      "Epoch 194/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6870 - val_loss: 17.6099\n",
      "Epoch 195/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6894 - val_loss: 17.6098\n",
      "Epoch 196/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6840 - val_loss: 17.6098\n",
      "Epoch 197/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6871 - val_loss: 17.6097\n",
      "Epoch 198/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6865 - val_loss: 17.6096\n",
      "Epoch 199/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.6861 - val_loss: 17.6096\n",
      "Epoch 200/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.6873 - val_loss: 17.6095\n",
      "Saving model...\n",
      "Done.\n",
      "hey let's loading the numpy data for speaker s1\n",
      "reading x_raw: \n",
      "(90, 16, 50, 100, 3)\n",
      "reading y_raw: \n",
      "(90, 6)\n",
      "reading word_len_list: \n",
      "(90,)\n",
      "reading input_len_list: \n",
      "(90,)\n",
      "('training data shapes:', (90, 16, 50, 100, 3), (90, 6))\n",
      "\n",
      "after dense1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 16, 50, 100,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "padding1 (ZeroPadding3D)        (None, 16, 54, 104,  0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_22 (TimeDistri (None, 16, 27, 52, 3 2432        padding1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_23 (TimeDistri (None, 16, 13, 26, 3 0           time_distributed_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16, 13, 26, 3 0           time_distributed_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_24 (TimeDistri (None, 16, 7, 13, 32 25632       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_25 (TimeDistri (None, 16, 3, 6, 32) 0           time_distributed_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16, 3, 6, 32) 0           time_distributed_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_26 (TimeDistri (None, 16, 2, 3, 4)  3204        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_27 (TimeDistri (None, 16, 1, 1, 4)  0           time_distributed_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 1, 1, 4)  0           time_distributed_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_28 (TimeDistri (None, 16, 4)        0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 16, 512)      400896      time_distributed_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 16, 28)       14364       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 16, 28)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 446,528\n",
      "Trainable params: 446,528\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 57 samples, validate on 15 samples\n",
      "Epoch 1/200\n",
      "57/57 [==============================] - 4s 67ms/step - loss: 15.6077 - val_loss: 17.5836\n",
      "Epoch 2/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 15.6018 - val_loss: 17.5629\n",
      "Epoch 3/200\n",
      "57/57 [==============================] - 2s 41ms/step - loss: 15.5916 - val_loss: 17.5321\n",
      "Epoch 4/200\n",
      "57/57 [==============================] - 3s 46ms/step - loss: 15.5935 - val_loss: 17.4977\n",
      "Epoch 5/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.5748 - val_loss: 17.4530\n",
      "Epoch 6/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.5612 - val_loss: 17.4128\n",
      "Epoch 7/200\n",
      "57/57 [==============================] - 2s 43ms/step - loss: 15.5628 - val_loss: 17.3727\n",
      "Epoch 8/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.5246 - val_loss: 17.3392\n",
      "Epoch 9/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 15.5149 - val_loss: 17.3123\n",
      "Epoch 10/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 15.5089 - val_loss: 17.2845\n",
      "Epoch 11/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.5000 - val_loss: 17.2508\n",
      "Epoch 12/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.4936 - val_loss: 17.2160\n",
      "Epoch 13/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.4712 - val_loss: 17.1742\n",
      "Epoch 14/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 15.4247 - val_loss: 17.1322\n",
      "Epoch 15/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 15.3880 - val_loss: 17.0938\n",
      "Epoch 16/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.4076 - val_loss: 17.0524\n",
      "Epoch 17/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.3576 - val_loss: 17.0084\n",
      "Epoch 18/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.3179 - val_loss: 16.9577\n",
      "Epoch 19/200\n",
      "57/57 [==============================] - 2s 41ms/step - loss: 15.3055 - val_loss: 16.9065\n",
      "Epoch 20/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 15.2702 - val_loss: 16.8474\n",
      "Epoch 21/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.1690 - val_loss: 16.7911\n",
      "Epoch 22/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 15.1484 - val_loss: 16.7203\n",
      "Epoch 23/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 15.1207 - val_loss: 16.6467\n",
      "Epoch 24/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 15.0323 - val_loss: 16.5750\n",
      "Epoch 25/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 14.9556 - val_loss: 16.5150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 14.9229 - val_loss: 16.4666\n",
      "Epoch 27/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 14.7639 - val_loss: 16.3980\n",
      "Epoch 28/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 14.7221 - val_loss: 16.3184\n",
      "Epoch 29/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 14.6446 - val_loss: 16.2288\n",
      "Epoch 30/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 14.5824 - val_loss: 16.1370\n",
      "Epoch 31/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 14.4896 - val_loss: 16.0443\n",
      "Epoch 32/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 14.4725 - val_loss: 15.9511\n",
      "Epoch 33/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 14.3164 - val_loss: 15.8621\n",
      "Epoch 34/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 14.2394 - val_loss: 15.7772\n",
      "Epoch 35/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 14.2088 - val_loss: 15.6998\n",
      "Epoch 36/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 14.1602 - val_loss: 15.6290\n",
      "Epoch 37/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 14.0998 - val_loss: 15.5668\n",
      "Epoch 38/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 14.0677 - val_loss: 15.5142\n",
      "Epoch 39/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 14.0120 - val_loss: 15.4665\n",
      "Epoch 40/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.9191 - val_loss: 15.4221\n",
      "Epoch 41/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.8807 - val_loss: 15.3823\n",
      "Epoch 42/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.8477 - val_loss: 15.3480\n",
      "Epoch 43/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.8894 - val_loss: 15.3162\n",
      "Epoch 44/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.8082 - val_loss: 15.2876\n",
      "Epoch 45/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.7596 - val_loss: 15.2614\n",
      "Epoch 46/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.7890 - val_loss: 15.2380\n",
      "Epoch 47/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.7820 - val_loss: 15.2167\n",
      "Epoch 48/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.7050 - val_loss: 15.1982\n",
      "Epoch 49/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.7004 - val_loss: 15.1811\n",
      "Epoch 50/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.7052 - val_loss: 15.1660\n",
      "Epoch 51/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.7157 - val_loss: 15.1530\n",
      "Epoch 52/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.7079 - val_loss: 15.1414\n",
      "Epoch 53/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.7108 - val_loss: 15.1312\n",
      "Epoch 54/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.7008 - val_loss: 15.1209\n",
      "Epoch 55/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6832 - val_loss: 15.1121\n",
      "Epoch 56/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6540 - val_loss: 15.1042\n",
      "Epoch 57/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6411 - val_loss: 15.0967\n",
      "Epoch 58/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6117 - val_loss: 15.0902\n",
      "Epoch 59/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.6194 - val_loss: 15.0845\n",
      "Epoch 60/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.6409 - val_loss: 15.0794\n",
      "Epoch 61/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6269 - val_loss: 15.0747\n",
      "Epoch 62/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6035 - val_loss: 15.0705\n",
      "Epoch 63/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6044 - val_loss: 15.0668\n",
      "Epoch 64/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6151 - val_loss: 15.0634\n",
      "Epoch 65/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.6134 - val_loss: 15.0606\n",
      "Epoch 66/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6105 - val_loss: 15.0582\n",
      "Epoch 67/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6241 - val_loss: 15.0557\n",
      "Epoch 68/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.6150 - val_loss: 15.0535\n",
      "Epoch 69/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5935 - val_loss: 15.0516\n",
      "Epoch 70/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5961 - val_loss: 15.0498\n",
      "Epoch 71/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5984 - val_loss: 15.0481\n",
      "Epoch 72/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6028 - val_loss: 15.0465\n",
      "Epoch 73/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5954 - val_loss: 15.0449\n",
      "Epoch 74/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5845 - val_loss: 15.0435\n",
      "Epoch 75/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6067 - val_loss: 15.0423\n",
      "Epoch 76/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5763 - val_loss: 15.0413\n",
      "Epoch 77/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5895 - val_loss: 15.0403\n",
      "Epoch 78/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5815 - val_loss: 15.0395\n",
      "Epoch 79/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5812 - val_loss: 15.0383\n",
      "Epoch 80/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5858 - val_loss: 15.0372\n",
      "Epoch 81/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5913 - val_loss: 15.0361\n",
      "Epoch 82/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5721 - val_loss: 15.0351\n",
      "Epoch 83/200\n",
      "57/57 [==============================] - 3s 44ms/step - loss: 13.5773 - val_loss: 15.0342\n",
      "Epoch 84/200\n",
      "57/57 [==============================] - 3s 45ms/step - loss: 13.5757 - val_loss: 15.0331\n",
      "Epoch 85/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5953 - val_loss: 15.0321\n",
      "Epoch 86/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.6338 - val_loss: 15.0309\n",
      "Epoch 87/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5713 - val_loss: 15.0297\n",
      "Epoch 88/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5843 - val_loss: 15.0286\n",
      "Epoch 89/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5789 - val_loss: 15.0276\n",
      "Epoch 90/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5724 - val_loss: 15.0267\n",
      "Epoch 91/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5709 - val_loss: 15.0259\n",
      "Epoch 92/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5761 - val_loss: 15.0251\n",
      "Epoch 93/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5842 - val_loss: 15.0244\n",
      "Epoch 94/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5667 - val_loss: 15.0238\n",
      "Epoch 95/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5695 - val_loss: 15.0231\n",
      "Epoch 96/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5750 - val_loss: 15.0225\n",
      "Epoch 97/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5724 - val_loss: 15.0220\n",
      "Epoch 98/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5765 - val_loss: 15.0213\n",
      "Epoch 99/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5754 - val_loss: 15.0206\n",
      "Epoch 100/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5711 - val_loss: 15.0201\n",
      "Epoch 101/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5654 - val_loss: 15.0195\n",
      "Epoch 102/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5692 - val_loss: 15.0191\n",
      "Epoch 103/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5668 - val_loss: 15.0186\n",
      "Epoch 104/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5670 - val_loss: 15.0183\n",
      "Epoch 105/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5707 - val_loss: 15.0178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5668 - val_loss: 15.0174\n",
      "Epoch 107/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5741 - val_loss: 15.0171\n",
      "Epoch 108/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5741 - val_loss: 15.0167\n",
      "Epoch 109/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5693 - val_loss: 15.0164\n",
      "Epoch 110/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5681 - val_loss: 15.0160\n",
      "Epoch 111/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5642 - val_loss: 15.0157\n",
      "Epoch 112/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5628 - val_loss: 15.0154\n",
      "Epoch 113/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5642 - val_loss: 15.0151\n",
      "Epoch 114/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5738 - val_loss: 15.0149\n",
      "Epoch 115/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5607 - val_loss: 15.0146\n",
      "Epoch 116/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5669 - val_loss: 15.0143\n",
      "Epoch 117/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5714 - val_loss: 15.0139\n",
      "Epoch 118/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5614 - val_loss: 15.0136\n",
      "Epoch 119/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5658 - val_loss: 15.0133\n",
      "Epoch 120/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5626 - val_loss: 15.0130\n",
      "Epoch 121/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5680 - val_loss: 15.0128\n",
      "Epoch 122/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5589 - val_loss: 15.0125\n",
      "Epoch 123/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5653 - val_loss: 15.0123\n",
      "Epoch 124/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5665 - val_loss: 15.0120\n",
      "Epoch 125/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5634 - val_loss: 15.0118\n",
      "Epoch 126/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5589 - val_loss: 15.0116\n",
      "Epoch 127/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5717 - val_loss: 15.0115\n",
      "Epoch 128/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5639 - val_loss: 15.0113\n",
      "Epoch 129/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5612 - val_loss: 15.0111\n",
      "Epoch 130/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5599 - val_loss: 15.0110\n",
      "Epoch 131/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5629 - val_loss: 15.0108\n",
      "Epoch 132/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5628 - val_loss: 15.0107\n",
      "Epoch 133/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5603 - val_loss: 15.0105\n",
      "Epoch 134/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5604 - val_loss: 15.0104\n",
      "Epoch 135/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5650 - val_loss: 15.0103\n",
      "Epoch 136/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5617 - val_loss: 15.0101\n",
      "Epoch 137/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5588 - val_loss: 15.0100\n",
      "Epoch 138/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5592 - val_loss: 15.0099\n",
      "Epoch 139/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5616 - val_loss: 15.0098\n",
      "Epoch 140/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5602 - val_loss: 15.0096\n",
      "Epoch 141/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5601 - val_loss: 15.0095\n",
      "Epoch 142/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5602 - val_loss: 15.0094\n",
      "Epoch 143/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5614 - val_loss: 15.0093\n",
      "Epoch 144/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5625 - val_loss: 15.0092\n",
      "Epoch 145/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5606 - val_loss: 15.0091\n",
      "Epoch 146/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5624 - val_loss: 15.0090\n",
      "Epoch 147/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5600 - val_loss: 15.0089\n",
      "Epoch 148/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5614 - val_loss: 15.0088\n",
      "Epoch 149/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5674 - val_loss: 15.0087\n",
      "Epoch 150/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5616 - val_loss: 15.0086\n",
      "Epoch 151/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5543 - val_loss: 15.0085\n",
      "Epoch 152/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5574 - val_loss: 15.0084\n",
      "Epoch 153/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5588 - val_loss: 15.0083\n",
      "Epoch 154/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5602 - val_loss: 15.0081\n",
      "Epoch 155/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5591 - val_loss: 15.0080\n",
      "Epoch 156/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5567 - val_loss: 15.0079\n",
      "Epoch 157/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5673 - val_loss: 15.0077\n",
      "Epoch 158/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5582 - val_loss: 15.0076\n",
      "Epoch 159/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5565 - val_loss: 15.0074\n",
      "Epoch 160/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5567 - val_loss: 15.0073\n",
      "Epoch 161/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5576 - val_loss: 15.0071\n",
      "Epoch 162/200\n",
      "57/57 [==============================] - 2s 41ms/step - loss: 13.5616 - val_loss: 15.0070\n",
      "Epoch 163/200\n",
      "57/57 [==============================] - 3s 48ms/step - loss: 13.5669 - val_loss: 15.0069\n",
      "Epoch 164/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5593 - val_loss: 15.0068\n",
      "Epoch 165/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5581 - val_loss: 15.0066\n",
      "Epoch 166/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5597 - val_loss: 15.0065\n",
      "Epoch 167/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5571 - val_loss: 15.0064\n",
      "Epoch 168/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5576 - val_loss: 15.0063\n",
      "Epoch 169/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5662 - val_loss: 15.0062\n",
      "Epoch 170/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5595 - val_loss: 15.0060\n",
      "Epoch 171/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5605 - val_loss: 15.0059\n",
      "Epoch 172/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5572 - val_loss: 15.0058\n",
      "Epoch 173/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5602 - val_loss: 15.0057\n",
      "Epoch 174/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5575 - val_loss: 15.0056\n",
      "Epoch 175/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5566 - val_loss: 15.0055\n",
      "Epoch 176/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5579 - val_loss: 15.0054\n",
      "Epoch 177/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5572 - val_loss: 15.0052\n",
      "Epoch 178/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5533 - val_loss: 15.0052\n",
      "Epoch 179/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5558 - val_loss: 15.0051\n",
      "Epoch 180/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5565 - val_loss: 15.0050\n",
      "Epoch 181/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5576 - val_loss: 15.0049\n",
      "Epoch 182/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5571 - val_loss: 15.0048\n",
      "Epoch 183/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5576 - val_loss: 15.0047\n",
      "Epoch 184/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5598 - val_loss: 15.0046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5599 - val_loss: 15.0046\n",
      "Epoch 186/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5547 - val_loss: 15.0045\n",
      "Epoch 187/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5610 - val_loss: 15.0044\n",
      "Epoch 188/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5569 - val_loss: 15.0043\n",
      "Epoch 189/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5616 - val_loss: 15.0042\n",
      "Epoch 190/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5587 - val_loss: 15.0041\n",
      "Epoch 191/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5592 - val_loss: 15.0041\n",
      "Epoch 192/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5601 - val_loss: 15.0040\n",
      "Epoch 193/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5580 - val_loss: 15.0039\n",
      "Epoch 194/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5549 - val_loss: 15.0038\n",
      "Epoch 195/200\n",
      "57/57 [==============================] - 2s 39ms/step - loss: 13.5587 - val_loss: 15.0038\n",
      "Epoch 196/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5563 - val_loss: 15.0037\n",
      "Epoch 197/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5558 - val_loss: 15.0036\n",
      "Epoch 198/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5558 - val_loss: 15.0036\n",
      "Epoch 199/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5541 - val_loss: 15.0035\n",
      "Epoch 200/200\n",
      "57/57 [==============================] - 2s 40ms/step - loss: 13.5558 - val_loss: 15.0034\n",
      "Saving model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# train by loading data saved in load_data2\n",
    "for speaker in speakers:\n",
    "    \n",
    "    print(\"hey let's loading the numpy data for speaker \" + speaker)\n",
    "    \n",
    "    print(\"reading x_raw: \")\n",
    "    x_raw = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_x2.npy\")\n",
    "    x_raw = np.stack(x_raw, axis=0)\n",
    "    print(x_raw.shape)\n",
    "    \n",
    "    print(\"reading y_raw: \")\n",
    "    y_raw = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_y2.npy\")\n",
    "    print(y_raw.shape)\n",
    "    \n",
    "    print(\"reading word_len_list: \")\n",
    "    word_len_list = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_word_len_list2.npy\")\n",
    "    print(word_len_list.shape)\n",
    "    \n",
    "    print(\"reading input_len_list: \")\n",
    "    input_len_list = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_input_len_list2.npy\")\n",
    "    print(input_len_list.shape)\n",
    "    \n",
    "    print(\"training data shapes:\", x_raw.shape, y_raw.shape)\n",
    "    x_train, x_test, y_train, y_test, label_len_train, label_len_test, \\\n",
    "    input_len_train, input_len_test = train_test_split(x_raw, y_raw, word_len_list, input_len_list, test_size=0.2)\n",
    "\n",
    "    model = build_model(x_raw.shape[1:], 28, max_string_len = 10)\n",
    "\n",
    "    history = train(model, x_train, y_train, label_len_train, input_len_train, epochs=200)\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    model.save('model_'+ speaker +\".h5\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check loading time for each speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey let's loading the numpy data for speaker s13\n",
      "reading x_raw: \n",
      "(60, 16, 50, 100, 3)\n",
      "reading y_raw: \n",
      "(60, 6)\n",
      "reading word_len_list: \n",
      "(60,)\n",
      "reading input_len_list: \n",
      "(60,)\n",
      "hey let's loading the numpy data for speaker s1\n",
      "reading x_raw: \n",
      "(90, 16, 50, 100, 3)\n",
      "reading y_raw: \n",
      "(90, 6)\n",
      "reading word_len_list: \n",
      "(90,)\n",
      "reading input_len_list: \n",
      "(90,)\n",
      "('load data took', 0.1183469295501709)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for speaker in speakers:\n",
    "    \n",
    "    print(\"hey let's loading the numpy data for speaker \" + speaker)\n",
    "    print(\"reading x_raw: \")\n",
    "    x_raw = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_x2.npy\")\n",
    "    x_raws = x_raw\n",
    "    print(x_raw.shape)\n",
    "    print(\"reading y_raw: \")\n",
    "    y_raw = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_y2.npy\")\n",
    "    y_raws[speaker] = y_raw\n",
    "    print(y_raw.shape)\n",
    "    print(\"reading word_len_list: \")\n",
    "    word_len_list = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_word_len_list2.npy\")\n",
    "    print(word_len_list.shape)\n",
    "    print(\"reading input_len_list: \")\n",
    "    input_len_list = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_input_len_list2.npy\")\n",
    "\n",
    "    print(input_len_list.shape)\n",
    "\n",
    "end = time.time()\n",
    "print(\"load data took\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for specific speaker\n",
    "def load_data_for_speaker(datapath, speaker_id, verbose=False, num_samples=-1, ctc_encoding=False):\n",
    "    oh = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    counter = 0\n",
    "    done = False\n",
    "\n",
    "    max_len = 0\n",
    "    max_word_len = 0\n",
    "\n",
    "    word_len_list = []\n",
    "    input_len_list = []\n",
    "\n",
    "    x_raw = list()\n",
    "    y_raw = list()\n",
    "    \n",
    "    path = datapath + \"/\" + speaker_id\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):       \n",
    "        \n",
    "        for name in files:\n",
    "            if '.mpg' in name:\n",
    "                if verbose is True:\n",
    "                    print(\"reading: \" + root + \"/\" + name)\n",
    "\n",
    "                video = read_video(os.path.join(root, name), PREDICTOR_PATH)\n",
    "                alignments = read_align(os.path.join(root, '../align/', name.split(\".\")[0] + \".align\"))\n",
    "                \n",
    "                for start, stop, word in alignments:\n",
    "                    if word == 'sil' or word == 'sp':\n",
    "                        continue\n",
    "                    _, d1, d2, d3 = video[start:stop].shape\n",
    "                    \n",
    "                    if (len(x_raw) > 0):\n",
    "                        _, prev_d1, prev_d2, prev_d3 = x_raw[-1].shape\n",
    "                        if (d1, d2, d3) != (prev_d1, prev_d2, prev_d3):\n",
    "                            if verbose is True:\n",
    "                                print(\"different size, skip\")\n",
    "                            continue\n",
    "                    \n",
    "                    x_raw.append(video[start:stop])\n",
    "                    y_raw.append(word)\n",
    "                    \n",
    "                    max_word_len = max(max_word_len, len(word))\n",
    "                    max_len = max(max_len, stop-start)\n",
    "                    word_len_list.append(len(word))\n",
    "                    input_len_list.append(stop-start)\n",
    "                    \n",
    "                    counter += 1\n",
    "                    if counter == num_samples:\n",
    "                        done = True\n",
    "                        break\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if not ctc_encoding:\n",
    "        y_raw = le.fit_transform(y_raw)\n",
    "        y = oh.fit_transform(y_raw.reshape(-1, 1)).todense()\n",
    "\n",
    "\n",
    "    for i in range(len(x_raw)):\n",
    "        result = np.zeros((max_len, x_raw[i].shape[1], x_raw[i].shape[2], x_raw[i].shape[3]))\n",
    "        result[:x_raw[i].shape[0], :x_raw[i].shape[1], :x_raw[i].shape[2], :x_raw[i].shape[3]] = x_raw[i]\n",
    "        x_raw[i] = result\n",
    "        \n",
    "        \n",
    "        if ctc_encoding:\n",
    "            res = np.ones(max_word_len) * -1\n",
    "            enc = np.array(text_to_labels(y_raw[i]))\n",
    "            res[:enc.shape[0]] = enc\n",
    "            y_raw[i] = res\n",
    "\n",
    "    \n",
    "    \n",
    "    np_save = SAVE_NUMPY_PATH + \"/\" + speaker_id\n",
    "    np.save(np_save + \"_x\", x_raw) \n",
    "    np.save(np_save + \"_y\", y_raw)\n",
    "    np.save(np_save + \"_word_len_list\", np.array(word_len_list))\n",
    "    np.save(np_save + \"_input_len_list\", np.array(input_len_list))\n",
    "        \n",
    "#     if ctc_encoding:\n",
    "#         y = np.stack(y_raw, axis=0)\n",
    "\n",
    "#     x = np.stack(x_raw, axis=0)\n",
    "    return speaker_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae2p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq9s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaezn.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak4n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae1s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq8n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak7a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak5s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae3a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak6p.mpg\n"
     ]
    }
   ],
   "source": [
    "speakers = load_data_for_speaker(DATA_PATH, 's13', verbose=True, num_samples=150, ctc_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey let's loading the numpy data for speaker s13\n",
      "reading x_raw: \n",
      "(60, 16, 50, 100, 3)\n",
      "reading y_raw: \n",
      "(60, 6)\n",
      "reading word_len_list: \n",
      "(60,)\n",
      "reading input_len_list: \n",
      "(60,)\n",
      "('load data took', 0.09496402740478516)\n",
      "('training data shapes:', (60, 16, 50, 100, 3), (60, 6))\n",
      "\n",
      "after dense1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 16, 50, 100,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "padding1 (ZeroPadding3D)        (None, 16, 54, 104,  0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_36 (TimeDistri (None, 16, 27, 52, 3 2432        padding1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_37 (TimeDistri (None, 16, 13, 26, 3 0           time_distributed_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 13, 26, 3 0           time_distributed_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_38 (TimeDistri (None, 16, 7, 13, 32 25632       dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_39 (TimeDistri (None, 16, 3, 6, 32) 0           time_distributed_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 3, 6, 32) 0           time_distributed_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_40 (TimeDistri (None, 16, 2, 3, 4)  3204        dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_41 (TimeDistri (None, 16, 1, 1, 4)  0           time_distributed_40[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 1, 1, 4)  0           time_distributed_41[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_42 (TimeDistri (None, 16, 4)        0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 16, 512)      400896      time_distributed_42[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 16, 28)       14364       bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 16, 28)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 446,528\n",
      "Trainable params: 446,528\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 38 samples, validate on 10 samples\n",
      "Epoch 1/200\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 19.4869 - val_loss: 18.5571\n",
      "Epoch 2/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.4678 - val_loss: 18.5541\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.4528 - val_loss: 18.5465\n",
      "Epoch 4/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.4427 - val_loss: 18.5387\n",
      "Epoch 5/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.4512 - val_loss: 18.5310\n",
      "Epoch 6/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.4152 - val_loss: 18.5227\n",
      "Epoch 7/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.4165 - val_loss: 18.5126\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.3813 - val_loss: 18.5016\n",
      "Epoch 9/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.3717 - val_loss: 18.4911\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.3573 - val_loss: 18.4796\n",
      "Epoch 11/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.3175 - val_loss: 18.4659\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.3253 - val_loss: 18.4496\n",
      "Epoch 13/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.2416 - val_loss: 18.4299\n",
      "Epoch 14/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.2839 - val_loss: 18.4065\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 19.2143 - val_loss: 18.3800\n",
      "Epoch 16/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.1807 - val_loss: 18.3529\n",
      "Epoch 17/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.1186 - val_loss: 18.3220\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 19.0698 - val_loss: 18.2904\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 19.0162 - val_loss: 18.2578\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.9801 - val_loss: 18.2195\n",
      "Epoch 21/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.8273 - val_loss: 18.1759\n",
      "Epoch 22/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.7864 - val_loss: 18.1289\n",
      "Epoch 23/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.6301 - val_loss: 18.0789\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.6603 - val_loss: 18.0225\n",
      "Epoch 25/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 18.5199 - val_loss: 17.9507\n",
      "Epoch 26/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.3224 - val_loss: 17.8789\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.1738 - val_loss: 17.8191\n",
      "Epoch 28/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.2566 - val_loss: 17.7581\n",
      "Epoch 29/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 18.0968 - val_loss: 17.6943\n",
      "Epoch 30/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 17.9170 - val_loss: 17.6213\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.8692 - val_loss: 17.5428\n",
      "Epoch 32/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.6347 - val_loss: 17.4436\n",
      "Epoch 33/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.4514 - val_loss: 17.3318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.3532 - val_loss: 17.2170\n",
      "Epoch 35/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.3003 - val_loss: 17.0985\n",
      "Epoch 36/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.1073 - val_loss: 16.9859\n",
      "Epoch 37/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 17.0492 - val_loss: 16.8691\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.8877 - val_loss: 16.7517\n",
      "Epoch 39/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.8653 - val_loss: 16.6440\n",
      "Epoch 40/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.7892 - val_loss: 16.5449\n",
      "Epoch 41/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.7285 - val_loss: 16.4492\n",
      "Epoch 42/200\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 16.6263 - val_loss: 16.3649\n",
      "Epoch 43/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.5007 - val_loss: 16.2911\n",
      "Epoch 44/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.4677 - val_loss: 16.2261\n",
      "Epoch 45/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.3616 - val_loss: 16.1688\n",
      "Epoch 46/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.3839 - val_loss: 16.1184\n",
      "Epoch 47/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.2717 - val_loss: 16.0725\n",
      "Epoch 48/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.2704 - val_loss: 16.0309\n",
      "Epoch 49/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.2494 - val_loss: 15.9947\n",
      "Epoch 50/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.2544 - val_loss: 15.9619\n",
      "Epoch 51/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.2176 - val_loss: 15.9324\n",
      "Epoch 52/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.1853 - val_loss: 15.9058\n",
      "Epoch 53/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.1885 - val_loss: 15.8823\n",
      "Epoch 54/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.0985 - val_loss: 15.8609\n",
      "Epoch 55/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.1566 - val_loss: 15.8413\n",
      "Epoch 56/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.1476 - val_loss: 15.8230\n",
      "Epoch 57/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0935 - val_loss: 15.8063\n",
      "Epoch 58/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.1115 - val_loss: 15.7902\n",
      "Epoch 59/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0899 - val_loss: 15.7758\n",
      "Epoch 60/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0800 - val_loss: 15.7625\n",
      "Epoch 61/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0570 - val_loss: 15.7507\n",
      "Epoch 62/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0843 - val_loss: 15.7404\n",
      "Epoch 63/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0309 - val_loss: 15.7311\n",
      "Epoch 64/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0377 - val_loss: 15.7225\n",
      "Epoch 65/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0374 - val_loss: 15.7148\n",
      "Epoch 66/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0312 - val_loss: 15.7078\n",
      "Epoch 67/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0168 - val_loss: 15.7016\n",
      "Epoch 68/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0159 - val_loss: 15.6961\n",
      "Epoch 69/200\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 16.0430 - val_loss: 15.6910\n",
      "Epoch 70/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.0180 - val_loss: 15.6863\n",
      "Epoch 71/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0166 - val_loss: 15.6819\n",
      "Epoch 72/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0081 - val_loss: 15.6777\n",
      "Epoch 73/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0419 - val_loss: 15.6737\n",
      "Epoch 74/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 16.0191 - val_loss: 15.6700\n",
      "Epoch 75/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9905 - val_loss: 15.6665\n",
      "Epoch 76/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0024 - val_loss: 15.6633\n",
      "Epoch 77/200\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 15.9972 - val_loss: 15.6603\n",
      "Epoch 78/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9861 - val_loss: 15.6576\n",
      "Epoch 79/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9879 - val_loss: 15.6549\n",
      "Epoch 80/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0080 - val_loss: 15.6524\n",
      "Epoch 81/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9993 - val_loss: 15.6501\n",
      "Epoch 82/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9894 - val_loss: 15.6479\n",
      "Epoch 83/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9751 - val_loss: 15.6458\n",
      "Epoch 84/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9790 - val_loss: 15.6439\n",
      "Epoch 85/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0013 - val_loss: 15.6422\n",
      "Epoch 86/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9807 - val_loss: 15.6405\n",
      "Epoch 87/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9917 - val_loss: 15.6390\n",
      "Epoch 88/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 16.0034 - val_loss: 15.6375\n",
      "Epoch 89/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9742 - val_loss: 15.6361\n",
      "Epoch 90/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9872 - val_loss: 15.6348\n",
      "Epoch 91/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9892 - val_loss: 15.6335\n",
      "Epoch 92/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9923 - val_loss: 15.6322\n",
      "Epoch 93/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9735 - val_loss: 15.6310\n",
      "Epoch 94/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9766 - val_loss: 15.6299\n",
      "Epoch 95/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.9679 - val_loss: 15.6288\n",
      "Epoch 96/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9763 - val_loss: 15.6278\n",
      "Epoch 97/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9742 - val_loss: 15.6269\n",
      "Epoch 98/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9672 - val_loss: 15.6259\n",
      "Epoch 99/200\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 15.9700 - val_loss: 15.6250\n",
      "Epoch 100/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9764 - val_loss: 15.6241\n",
      "Epoch 101/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9722 - val_loss: 15.6232\n",
      "Epoch 102/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9828 - val_loss: 15.6223\n",
      "Epoch 103/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9606 - val_loss: 15.6215\n",
      "Epoch 104/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9649 - val_loss: 15.6207\n",
      "Epoch 105/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9657 - val_loss: 15.6200\n",
      "Epoch 106/200\n",
      "38/38 [==============================] - 2s 42ms/step - loss: 15.9799 - val_loss: 15.6193\n",
      "Epoch 107/200\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 15.9672 - val_loss: 15.6186\n",
      "Epoch 108/200\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 15.9831 - val_loss: 15.6179\n",
      "Epoch 109/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9619 - val_loss: 15.6172\n",
      "Epoch 110/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.9656 - val_loss: 15.6165\n",
      "Epoch 111/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9880 - val_loss: 15.6158\n",
      "Epoch 112/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9641 - val_loss: 15.6153\n",
      "Epoch 113/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9724 - val_loss: 15.6147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9603 - val_loss: 15.6141\n",
      "Epoch 115/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9572 - val_loss: 15.6136\n",
      "Epoch 116/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9622 - val_loss: 15.6130\n",
      "Epoch 117/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9588 - val_loss: 15.6125\n",
      "Epoch 118/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9875 - val_loss: 15.6119\n",
      "Epoch 119/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9623 - val_loss: 15.6114\n",
      "Epoch 120/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9616 - val_loss: 15.6109\n",
      "Epoch 121/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9588 - val_loss: 15.6104\n",
      "Epoch 122/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9554 - val_loss: 15.6100\n",
      "Epoch 123/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9605 - val_loss: 15.6095\n",
      "Epoch 124/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9693 - val_loss: 15.6090\n",
      "Epoch 125/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9673 - val_loss: 15.6086\n",
      "Epoch 126/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9621 - val_loss: 15.6081\n",
      "Epoch 127/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9685 - val_loss: 15.6076\n",
      "Epoch 128/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9571 - val_loss: 15.6071\n",
      "Epoch 129/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9643 - val_loss: 15.6067\n",
      "Epoch 130/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9761 - val_loss: 15.6063\n",
      "Epoch 131/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9679 - val_loss: 15.6058\n",
      "Epoch 132/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9599 - val_loss: 15.6054\n",
      "Epoch 133/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9559 - val_loss: 15.6049\n",
      "Epoch 134/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9562 - val_loss: 15.6045\n",
      "Epoch 135/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9549 - val_loss: 15.6042\n",
      "Epoch 136/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9597 - val_loss: 15.6038\n",
      "Epoch 137/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9603 - val_loss: 15.6034\n",
      "Epoch 138/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9570 - val_loss: 15.6030\n",
      "Epoch 139/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9661 - val_loss: 15.6026\n",
      "Epoch 140/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9551 - val_loss: 15.6022\n",
      "Epoch 141/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9662 - val_loss: 15.6019\n",
      "Epoch 142/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9625 - val_loss: 15.6015\n",
      "Epoch 143/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9501 - val_loss: 15.6012\n",
      "Epoch 144/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9631 - val_loss: 15.6008\n",
      "Epoch 145/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9509 - val_loss: 15.6005\n",
      "Epoch 146/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9563 - val_loss: 15.6002\n",
      "Epoch 147/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9525 - val_loss: 15.5999\n",
      "Epoch 148/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9547 - val_loss: 15.5995\n",
      "Epoch 149/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9541 - val_loss: 15.5992\n",
      "Epoch 150/200\n",
      "38/38 [==============================] - 2s 39ms/step - loss: 15.9602 - val_loss: 15.5989\n",
      "Epoch 151/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9514 - val_loss: 15.5986\n",
      "Epoch 152/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9543 - val_loss: 15.5983\n",
      "Epoch 153/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9522 - val_loss: 15.5980\n",
      "Epoch 154/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9552 - val_loss: 15.5978\n",
      "Epoch 155/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9538 - val_loss: 15.5975\n",
      "Epoch 156/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9554 - val_loss: 15.5972\n",
      "Epoch 157/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.9498 - val_loss: 15.5969\n",
      "Epoch 158/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9483 - val_loss: 15.5967\n",
      "Epoch 159/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9509 - val_loss: 15.5964\n",
      "Epoch 160/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9492 - val_loss: 15.5962\n",
      "Epoch 161/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9521 - val_loss: 15.5959\n",
      "Epoch 162/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9469 - val_loss: 15.5957\n",
      "Epoch 163/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9478 - val_loss: 15.5955\n",
      "Epoch 164/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9506 - val_loss: 15.5953\n",
      "Epoch 165/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9778 - val_loss: 15.5950\n",
      "Epoch 166/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9506 - val_loss: 15.5948\n",
      "Epoch 167/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9528 - val_loss: 15.5946\n",
      "Epoch 168/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.9472 - val_loss: 15.5944\n",
      "Epoch 169/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.9571 - val_loss: 15.5942\n",
      "Epoch 170/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.9509 - val_loss: 15.5940\n",
      "Epoch 171/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9481 - val_loss: 15.5938\n",
      "Epoch 172/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9504 - val_loss: 15.5936\n",
      "Epoch 173/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9472 - val_loss: 15.5934\n",
      "Epoch 174/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.9866 - val_loss: 15.5932\n",
      "Epoch 175/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9518 - val_loss: 15.5931\n",
      "Epoch 176/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9497 - val_loss: 15.5929\n",
      "Epoch 177/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9478 - val_loss: 15.5928\n",
      "Epoch 178/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9537 - val_loss: 15.5926\n",
      "Epoch 179/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9457 - val_loss: 15.5924\n",
      "Epoch 180/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9496 - val_loss: 15.5923\n",
      "Epoch 181/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9474 - val_loss: 15.5921\n",
      "Epoch 182/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9499 - val_loss: 15.5920\n",
      "Epoch 183/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9454 - val_loss: 15.5918\n",
      "Epoch 184/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9485 - val_loss: 15.5917\n",
      "Epoch 185/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9532 - val_loss: 15.5915\n",
      "Epoch 186/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9455 - val_loss: 15.5914\n",
      "Epoch 187/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9486 - val_loss: 15.5912\n",
      "Epoch 188/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9541 - val_loss: 15.5911\n",
      "Epoch 189/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9451 - val_loss: 15.5909\n",
      "Epoch 190/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9500 - val_loss: 15.5908\n",
      "Epoch 191/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9444 - val_loss: 15.5906\n",
      "Epoch 192/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9452 - val_loss: 15.5905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9482 - val_loss: 15.5904\n",
      "Epoch 194/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.9467 - val_loss: 15.5902\n",
      "Epoch 195/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9527 - val_loss: 15.5901\n",
      "Epoch 196/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9483 - val_loss: 15.5899\n",
      "Epoch 197/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9477 - val_loss: 15.5898\n",
      "Epoch 198/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9462 - val_loss: 15.5897\n",
      "Epoch 199/200\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 15.9651 - val_loss: 15.5895\n",
      "Epoch 200/200\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 15.9452 - val_loss: 15.5894\n",
      "Saving model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if speakers:\n",
    "    start = time.time()\n",
    "    print(\"hey let's loading the numpy data for speaker \" + speakers)\n",
    "    \n",
    "    print(\"reading x_raw: \")\n",
    "    x_raw = np.load(SAVE_NUMPY_PATH + \"/\" + speakers + \"_x.npy\")\n",
    "    x_raw = np.stack(x_raw, axis=0)\n",
    "    print(x_raw.shape)\n",
    "    \n",
    "    print(\"reading y_raw: \")\n",
    "    y_raw = np.load(SAVE_NUMPY_PATH + \"/\" + speakers + \"_y.npy\")\n",
    "    print(y_raw.shape)\n",
    "    \n",
    "    print(\"reading word_len_list: \")\n",
    "    word_len_list = np.load(SAVE_NUMPY_PATH + \"/\" + speakers + \"_word_len_list.npy\")\n",
    "    print(word_len_list.shape)\n",
    "    \n",
    "    print(\"reading input_len_list: \")\n",
    "    input_len_list = np.load(SAVE_NUMPY_PATH + \"/\" + speakers + \"_input_len_list.npy\")\n",
    "    print(input_len_list.shape)\n",
    "    end = time.time()\n",
    "    print(\"load data took\", end-start)\n",
    "    print(\"training data shapes:\", x_raw.shape, y_raw.shape)\n",
    "    x_train, x_test, y_train, y_test, label_len_train, label_len_test, \\\n",
    "    input_len_train, input_len_test = train_test_split(x_raw, y_raw, word_len_list, input_len_list, test_size=0.2)\n",
    "\n",
    "    model = build_model(x_raw.shape[1:], 28, max_string_len = 10)\n",
    "\n",
    "    history = train(model, x_train, y_train, label_len_train, input_len_train, epochs=200)\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    model.save('model_'+ speakers +\".h5\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
