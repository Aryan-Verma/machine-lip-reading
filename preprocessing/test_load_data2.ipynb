{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding3D\n",
    "from keras.layers.core import Lambda, Dropout, Flatten, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "DATA_PATH = CURRENT_PATH + '/../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    import tensorflow as tf\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # From Keras example image_ocr.py:\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    # y_pred = y_pred[:, 2:, :]\n",
    "    label_length = K.cast(tf.squeeze(label_length),'int32')\n",
    "    input_length = K.cast(tf.squeeze(input_length),'int32')\n",
    "    labels = K.ctc_label_dense_to_sparse(labels, label_length)\n",
    "    #y_pred = y_pred[:, :, :]\n",
    "    #return K.ctc_batch_cost(labels, y_pred, input_length, label_length, ignore_longer_outputs_than_inputs=True)\n",
    "    return tf.nn.ctc_loss(labels, y_pred, input_length, ctc_merge_repeated=False,\n",
    "                         ignore_longer_outputs_than_inputs = True, time_major = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTC(name, args):\n",
    "\treturn Lambda(ctc_lambda_func, output_shape=(1,), name=name)(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, output_size = 28, max_string_len = 10):\n",
    "    # model = Sequential()\n",
    "    input_data = Input(name='the_input', shape=input_size, dtype='float32')\n",
    "    x = ZeroPadding3D(padding=(0,2,2), name='padding1')(input_data)\n",
    "    x = TimeDistributed(Conv2D(filters = 32, kernel_size = 5, strides = (2,2),\n",
    "                             padding = 'same', activation = 'relu'))(x)\n",
    "    print\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(filters=32, kernel_size=5, strides=(2, 2),\n",
    "                               padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(filters=4, kernel_size=5, strides=(2, 2),\n",
    "                               padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    input_lstm = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    x_lstm = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(input_lstm)\n",
    "    x_lstm = Dense(output_size, kernel_initializer='he_normal', name='dense1')(x_lstm)\n",
    "    print(\"after dense1\")\n",
    "    y_pred = Activation('softmax', name='softmax')(x_lstm)\n",
    "\n",
    "    labels = Input(name='the_labels', shape = [max_string_len], dtype='int32')\n",
    "    input_length = Input(name = 'input_length', shape =[1], dtype = 'int32')\n",
    "    label_length = Input(name = 'label_length', shape = [1], dtype = 'int32')\n",
    "    loss = CTC('ctc',[y_pred, labels, input_length, label_length])\n",
    "    model = Model(inputs=[input_data, labels, label_length, input_length],\n",
    "                  outputs = loss)\n",
    "    model.summary()\n",
    "    # Build model here...\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_labels(labels, max_string_len):\n",
    "    padding = np.ones((labels.shape[0], max_string_len - labels.shape[1])) * -1\n",
    "    return np.concatenate((labels, padding), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, label_len_train, input_len_train, batch_size=256, epochs=100, val_train_ratio=0.2):\n",
    "    max_string_len = 10\n",
    "    if y_train.shape[1] != max_string_len:\n",
    "        y_train = pad_labels(y_train, max_string_len)\n",
    "\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)\n",
    "    history = model.fit(x = {'the_input':x_train, 'the_labels':y_train, 'label_length':label_len_train,\n",
    "                             'input_length':input_len_train}, y = {'ctc': np.zeros([x_train.shape[0]])},\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=val_train_ratio,\n",
    "                        shuffle=True,\n",
    "                        verbose=1)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    oh = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    x = list()\n",
    "    y = list()\n",
    "    t = list()\n",
    "    print(\"loading images...\")\n",
    "    for i, (img, words) in enumerate(load_data(DATA_PATH, verbose=False, framebyframe=False)):\n",
    "        if img.shape[0] != 75:\n",
    "            continue\n",
    "        x.append(img)\n",
    "        y.append(words)\n",
    "\n",
    "        t += words.tolist()\n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "    t = le.fit_transform(t)\n",
    "    oh.fit(t.reshape(-1, 1))\n",
    "\n",
    "    print(\"convering to np array...\")\n",
    "    x = np.stack(x, axis=0)\n",
    "\n",
    "    print(\"transforming y...\")\n",
    "    for i in range(len(y)):\n",
    "        y_ = le.transform(y[i])\n",
    "        y[i] = np.asarray(oh.transform(y_.reshape(-1, 1)).todense())\n",
    "    y = np.stack(y, axis=0)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new speaker added: s13\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae2p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq9s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaezn.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak4n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae1s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq8n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak7a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak5s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae3a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak6p.mpg\n",
      "new speaker added: s1\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbifzp.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbizzn.mpg\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbal8p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbbf9a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbas2p.mpg\n"
     ]
    }
   ],
   "source": [
    "SAVE_NUMPY_PATH = CURRENT_PATH + '/../data/numpy_results/'\n",
    "\n",
    "speakers = load_data(DATA_PATH, verbose=True, num_samples=80, ctc_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training data shapes:', (80, 16, 50, 100, 3), (80, 6))\n",
      "\n",
      "after dense1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 16, 50, 100,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "padding1 (ZeroPadding3D)        (None, 16, 54, 104,  0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 16, 27, 52, 3 2432        padding1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 16, 13, 26, 3 0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16, 13, 26, 3 0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 16, 7, 13, 32 25632       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 16, 3, 6, 32) 0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16, 3, 6, 32) 0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 16, 2, 3, 4)  3204        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 16, 1, 1, 4)  0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 1, 1, 4)  0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 16, 4)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 16, 512)      400896      time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 16, 28)       14364       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 16, 28)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 446,528\n",
      "Trainable params: 446,528\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 51 samples, validate on 13 samples\n",
      "Epoch 1/100\n",
      "51/51 [==============================] - 3s 60ms/step - loss: 17.3966 - val_loss: 20.8072\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 17.3603 - val_loss: 20.7422\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 17.3265 - val_loss: 20.6468\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 17.2922 - val_loss: 20.5430\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 17.2723 - val_loss: 20.4447\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 17.2098 - val_loss: 20.3495\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 17.1532 - val_loss: 20.2449\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 17.1066 - val_loss: 20.1408\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 17.0475 - val_loss: 20.0321\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.9993 - val_loss: 19.9114\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 16.9117 - val_loss: 19.7877\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.8455 - val_loss: 19.6568\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.7768 - val_loss: 19.5180\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.6837 - val_loss: 19.3800\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.5874 - val_loss: 19.2379\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.4491 - val_loss: 19.0936\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 16.3826 - val_loss: 18.9510\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 16.2972 - val_loss: 18.8136\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.1800 - val_loss: 18.6814\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 16.0716 - val_loss: 18.5548\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.9849 - val_loss: 18.4426\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.9408 - val_loss: 18.3326\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 15.8090 - val_loss: 18.2412\n",
      "Epoch 24/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.7543 - val_loss: 18.1563\n",
      "Epoch 25/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.6522 - val_loss: 18.0726\n",
      "Epoch 26/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.6060 - val_loss: 17.9959\n",
      "Epoch 27/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.5429 - val_loss: 17.9210\n",
      "Epoch 28/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.5086 - val_loss: 17.8522\n",
      "Epoch 29/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.4189 - val_loss: 17.7920\n",
      "Epoch 30/100\n",
      "51/51 [==============================] - 2s 40ms/step - loss: 15.3162 - val_loss: 17.7371\n",
      "Epoch 31/100\n",
      "51/51 [==============================] - 2s 47ms/step - loss: 15.3425 - val_loss: 17.6871\n",
      "Epoch 32/100\n",
      "51/51 [==============================] - 2s 41ms/step - loss: 15.1840 - val_loss: 17.6396\n",
      "Epoch 33/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.2175 - val_loss: 17.5954\n",
      "Epoch 34/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.1550 - val_loss: 17.5560\n",
      "Epoch 35/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.1742 - val_loss: 17.5191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.1430 - val_loss: 17.4848\n",
      "Epoch 37/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.1448 - val_loss: 17.4540\n",
      "Epoch 38/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.1063 - val_loss: 17.4264\n",
      "Epoch 39/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.0708 - val_loss: 17.4018\n",
      "Epoch 40/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.0303 - val_loss: 17.3798\n",
      "Epoch 41/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 15.0064 - val_loss: 17.3610\n",
      "Epoch 42/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9974 - val_loss: 17.3454\n",
      "Epoch 43/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9695 - val_loss: 17.3325\n",
      "Epoch 44/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9716 - val_loss: 17.3217\n",
      "Epoch 45/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9547 - val_loss: 17.3124\n",
      "Epoch 46/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.9544 - val_loss: 17.3043\n",
      "Epoch 47/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9426 - val_loss: 17.2974\n",
      "Epoch 48/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9453 - val_loss: 17.2913\n",
      "Epoch 49/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9412 - val_loss: 17.2858\n",
      "Epoch 50/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.9368 - val_loss: 17.2809\n",
      "Epoch 51/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9424 - val_loss: 17.2764\n",
      "Epoch 52/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9270 - val_loss: 17.2724\n",
      "Epoch 53/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.9193 - val_loss: 17.2688\n",
      "Epoch 54/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9173 - val_loss: 17.2656\n",
      "Epoch 55/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9208 - val_loss: 17.2627\n",
      "Epoch 56/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9302 - val_loss: 17.2600\n",
      "Epoch 57/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9072 - val_loss: 17.2575\n",
      "Epoch 58/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9154 - val_loss: 17.2554\n",
      "Epoch 59/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8990 - val_loss: 17.2534\n",
      "Epoch 60/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9016 - val_loss: 17.2516\n",
      "Epoch 61/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9120 - val_loss: 17.2499\n",
      "Epoch 62/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9003 - val_loss: 17.2483\n",
      "Epoch 63/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8988 - val_loss: 17.2468\n",
      "Epoch 64/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8893 - val_loss: 17.2453\n",
      "Epoch 65/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8947 - val_loss: 17.2440\n",
      "Epoch 66/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8984 - val_loss: 17.2428\n",
      "Epoch 67/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.9033 - val_loss: 17.2417\n",
      "Epoch 68/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8881 - val_loss: 17.2405\n",
      "Epoch 69/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8798 - val_loss: 17.2395\n",
      "Epoch 70/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8847 - val_loss: 17.2384\n",
      "Epoch 71/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8909 - val_loss: 17.2373\n",
      "Epoch 72/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8834 - val_loss: 17.2362\n",
      "Epoch 73/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8953 - val_loss: 17.2351\n",
      "Epoch 74/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8848 - val_loss: 17.2340\n",
      "Epoch 75/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8729 - val_loss: 17.2330\n",
      "Epoch 76/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8816 - val_loss: 17.2321\n",
      "Epoch 77/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8820 - val_loss: 17.2312\n",
      "Epoch 78/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8832 - val_loss: 17.2303\n",
      "Epoch 79/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8758 - val_loss: 17.2295\n",
      "Epoch 80/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8852 - val_loss: 17.2287\n",
      "Epoch 81/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8762 - val_loss: 17.2279\n",
      "Epoch 82/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8747 - val_loss: 17.2271\n",
      "Epoch 83/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8677 - val_loss: 17.2264\n",
      "Epoch 84/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8812 - val_loss: 17.2257\n",
      "Epoch 85/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8673 - val_loss: 17.2251\n",
      "Epoch 86/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8825 - val_loss: 17.2245\n",
      "Epoch 87/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8883 - val_loss: 17.2240\n",
      "Epoch 88/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8789 - val_loss: 17.2234\n",
      "Epoch 89/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8720 - val_loss: 17.2229\n",
      "Epoch 90/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8742 - val_loss: 17.2224\n",
      "Epoch 91/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8733 - val_loss: 17.2219\n",
      "Epoch 92/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8744 - val_loss: 17.2215\n",
      "Epoch 93/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8651 - val_loss: 17.2211\n",
      "Epoch 94/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8705 - val_loss: 17.2207\n",
      "Epoch 95/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8746 - val_loss: 17.2203\n",
      "Epoch 96/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8746 - val_loss: 17.2200\n",
      "Epoch 97/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8715 - val_loss: 17.2196\n",
      "Epoch 98/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8713 - val_loss: 17.2193\n",
      "Epoch 99/100\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 14.8638 - val_loss: 17.2190\n",
      "Epoch 100/100\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 14.8673 - val_loss: 17.2187\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "x, y, label_len, input_len = speakers\n",
    "print(\"training data shapes:\", x.shape, y.shape)\n",
    "x_train, x_test, y_train, y_test, label_len_train, label_len_test, \\\n",
    "input_len_train, input_len_test = train_test_split(x, y, label_len, input_len, test_size=0.2)\n",
    "\n",
    "model = build_model(x.shape[1:], 28, max_string_len = 10)\n",
    "\n",
    "history = train(model, x_train, y_train, label_len_train, input_len_train, epochs=epochs)\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey let's loading the numpy data for speaker s13\n",
      "reading x_raw: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cd3fc53ec542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reading x_raw: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_NUMPY_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspeaker\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_x.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mx_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mx_raws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#     print(x_raw.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "\n",
    "word_len_lists = []\n",
    "input_len_lists = []\n",
    "\n",
    "x_raws = []\n",
    "y_raws = []\n",
    "\n",
    "start = time.time()\n",
    "for speaker in speakers:\n",
    "    \n",
    "    print(\"hey let's loading the numpy data for speaker \" + speaker)\n",
    "    print(\"reading x_raw: \")\n",
    "    x_raw = np.load(SAVE_NUMPY_PATH + speaker + \"_x.npy\")\n",
    "#     x_raw = np.stack(x_raw, axis=0)\n",
    "    x_raws.append(x_raw)\n",
    "#     print(x_raw.shape)\n",
    "    print(\"reading y_raw: \")\n",
    "    y_raw = np.load(SAVE_NUMPY_PATH + speaker + \"_y.npy\")\n",
    "    y_raws.append(y_raw)\n",
    "#     print(y_raw.shape)\n",
    "    print(\"reading word_len_list: \")\n",
    "    word_len_list = np.load(SAVE_NUMPY_PATH + speaker + \"_word_len_list.npy\")\n",
    "    np.append(word_len_lists, word_len_list)\n",
    "#     print(word_len_list.shape)\n",
    "    print(\"reading input_len_list: \")\n",
    "    input_len_list = np.load(SAVE_NUMPY_PATH + speaker + \"_input_len_list.npy\")\n",
    "    np.append(input_len_lists, input_len_list)\n",
    "#     print(input_len_list.shape)\n",
    "end = time.time()\n",
    "print(\"load data took\", end-start)\n",
    "x_raws = np.stack(x_raws, axis=0)\n",
    "# print()\n",
    "print(\"training data shapes:\", x_raws.shape, y_raws.shape)\n",
    "x_train, x_test, y_train, y_test, label_len_train, label_len_test, \\\n",
    "input_len_train, input_len_test = train_test_split(x_raws, y_raws, word_len_lists, input_len_lists, test_size=0.2)\n",
    "\n",
    "model = build_model(x_raw.shape[1:], 28, max_string_len = 10)\n",
    "\n",
    "history = train(model, x_train, y_train, label_len_train, input_len_train, epochs=epochs)\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model.save('model.h5')\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "# epochs = 100\n",
    "\n",
    "# start = time.time()\n",
    "# print(\"loading data\")\n",
    "# x, y, label_len, input_len = load_data(DATA_PATH, verbose=True, num_samples=18, ctc_encoding=True)\n",
    "# end = time.time()\n",
    "\n",
    "# print(\"load data took\", end-start)\n",
    "# print(\"training data shapes:\", x.shape, y.shape)\n",
    "# x_train, x_test, y_train, y_test, label_len_train, label_len_test, \\\n",
    "# input_len_train, input_len_test = train_test_split(x, y, label_len, input_len, test_size=0.2)\n",
    "\n",
    "# model = build_model(x.shape[1:], 28, max_string_len = 10)\n",
    "\n",
    "# history = train(model, x_train, y_train, label_len_train, input_len_train, epochs=epochs)\n",
    "\n",
    "# print(\"Saving model...\")\n",
    "# model.save('model.h5')\n",
    "\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from align import read_align\n",
    "from video import read_video\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import re\n",
    "\n",
    "# CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "DATA_PATH = CURRENT_PATH + '/../data'\n",
    "PREDICTOR_PATH = CURRENT_PATH + '/shape_predictor_68_face_landmarks.dat'\n",
    "SAVE_NUMPY_PATH = CURRENT_PATH + '/../data/numpy_results'\n",
    "\n",
    "\n",
    "def text_to_labels(text):\n",
    "    ret = []\n",
    "    for char in text:\n",
    "        if char >= 'a' and char <= 'z':\n",
    "            ret.append(ord(char) - ord('a'))\n",
    "        elif char == ' ':\n",
    "            ret.append(26)\n",
    "    return ret\n",
    "\n",
    "def labels_to_text(labels):\n",
    "# 26 is space, 27 is CTC blank char\n",
    "    text = ''\n",
    "    for c in labels:\n",
    "        if c >= 0 and c < 26:\n",
    "            text += chr(c + ord('a'))\n",
    "        elif c == 26:\n",
    "            text += ' '\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_data2(datapath, verbose=False, num_samples=-1, ctc_encoding=False):\n",
    "    oh = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    counter = 0\n",
    "    done = False\n",
    "\n",
    "    max_len = 0\n",
    "    max_word_len = 0\n",
    "\n",
    "    word_len_list = []\n",
    "    input_len_list = []\n",
    "\n",
    "    x_raw = list()\n",
    "    y_raw = list()\n",
    "    \n",
    "    pattern = re.compile(\"s[0-9]\") \n",
    "    speakers = []\n",
    "    x_raws = {}\n",
    "    y_raws = {}\n",
    "    word_len_lists = {}\n",
    "    input_len_lists = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(datapath):\n",
    "        check = root.split(\"/\")[-1]\n",
    "        match = pattern.findall(check)\n",
    "        if (len(match) > 0):\n",
    "            if check.index(match[0]) == 0 and check not in speakers:             \n",
    "                speakers.append(check)\n",
    "                x_raws[check] = []\n",
    "                y_raws[check] = []\n",
    "                word_len_lists[check] = []\n",
    "                input_len_lists[check] = []\n",
    "                if verbose:\n",
    "                    print(\"new speaker added: \" + check)            \n",
    "        \n",
    "        for name in files:\n",
    "            if '.mpg' in name:\n",
    "                if verbose is True:\n",
    "                    print(\"reading: \" + root + \"/\" + name)\n",
    "\n",
    "                video = read_video(os.path.join(root, name), PREDICTOR_PATH)\n",
    "                alignments = read_align(os.path.join(root, '../align/', name.split(\".\")[0] + \".align\"))\n",
    "                \n",
    "                for start, stop, word in alignments:\n",
    "                    if word == 'sil' or word == 'sp':\n",
    "                        continue\n",
    "                    \n",
    "#                     if verbose is True:\n",
    "#                         print(str(counter) + \": \" + str(start) + \"--\" + str(stop) + \": \" + word)\n",
    "                    \n",
    "                    _, d1, d2, d3 = video[start:stop].shape\n",
    "                    \n",
    "                    if (len(x_raw) > 0):\n",
    "                        _, prev_d1, prev_d2, prev_d3 = x_raw[-1].shape\n",
    "                        if (d1, d2, d3) != (prev_d1, prev_d2, prev_d3):\n",
    "                            if verbose is True:\n",
    "                                print(\"different size, skip\")\n",
    "                            continue\n",
    "                    \n",
    "                    x_raw.append(video[start:stop])\n",
    "                    y_raw.append(word)\n",
    "                    \n",
    "                    x_raws[speakers[-1]].append(video[start:stop])\n",
    "                    y_raws[speakers[-1]].append(word)\n",
    "                    \n",
    "                    max_word_len = max(max_word_len, len(word))\n",
    "                    max_len = max(max_len, stop-start)\n",
    "                    word_len_list.append(len(word))\n",
    "                    input_len_list.append(stop-start)\n",
    "                    \n",
    "                    word_len_lists[speakers[-1]].append(len(word))\n",
    "                    input_len_lists[speakers[-1]].append(stop-start)\n",
    "                    \n",
    "                    counter += 1\n",
    "                    if counter == num_samples:\n",
    "                        done = True\n",
    "                        break\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if not ctc_encoding:\n",
    "        y_raw = le.fit_transform(y_raw)\n",
    "        y = oh.fit_transform(y_raw.reshape(-1, 1)).todense()\n",
    "\n",
    "\n",
    "    for i in range(len(x_raw)):\n",
    "        result = np.zeros((max_len, x_raw[i].shape[1], x_raw[i].shape[2], x_raw[i].shape[3]))\n",
    "        result[:x_raw[i].shape[0], :x_raw[i].shape[1], :x_raw[i].shape[2], :x_raw[i].shape[3]] = x_raw[i]\n",
    "        x_raw[i] = result\n",
    "        \n",
    "        \n",
    "        if ctc_encoding:\n",
    "            res = np.ones(max_word_len) * -1\n",
    "            enc = np.array(text_to_labels(y_raw[i]))\n",
    "            res[:enc.shape[0]] = enc\n",
    "            y_raw[i] = res\n",
    "\n",
    "    \n",
    "    \n",
    "    for speaker in speakers:\n",
    "        \n",
    "        for i in range(len(x_raws[speaker])):\n",
    "            result = np.zeros((max_len, x_raws[speaker][i].shape[1], x_raws[speaker][i].shape[2], x_raws[speaker][i].shape[3]))\n",
    "            result[:x_raws[speaker][i].shape[0], :x_raws[speaker][i].shape[1], :x_raws[speaker][i].shape[2], :x_raws[speaker][i].shape[3]] = x_raws[speaker][i]\n",
    "            x_raws[speaker][i] = result\n",
    "\n",
    "\n",
    "            if ctc_encoding:\n",
    "                res = np.ones(max_word_len) * -1\n",
    "                enc = np.array(text_to_labels(y_raws[speaker][i]))\n",
    "                res[:enc.shape[0]] = enc\n",
    "                y_raws[speaker][i] = res\n",
    "\n",
    "        np_save = SAVE_NUMPY_PATH + \"/\" + speaker\n",
    "#         x_raws[speaker] = np.stack(x_raws[speaker], axis=0)\n",
    "        np.save(np_save + \"_x2\", x_raws[speaker]) \n",
    "#         if ctc_encoding:\n",
    "#             y = np.stack(y_raw, axis=0)\n",
    "        np.save(np_save + \"_y2\", y_raws[speaker])\n",
    "        np.save(np_save + \"_word_len_list2\", np.array(word_len_lists[speaker]))\n",
    "        np.save(np_save + \"_input_len_list2\", np.array(input_len_lists[speaker]))\n",
    "        \n",
    "    if ctc_encoding:\n",
    "        y = np.stack(y_raw, axis=0)\n",
    "\n",
    "    x = np.stack(x_raw, axis=0)\n",
    "    return speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new speaker added: s13\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae2p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq9s.mpg\n"
     ]
    }
   ],
   "source": [
    "speakers = load_data2(DATA_PATH, verbose=True, num_samples=12, ctc_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey let's loading the numpy data for speaker s13\n",
      "reading x_raw: \n",
      "(12, 11, 50, 100, 3)\n",
      "reading y_raw: \n",
      "(12, 6)\n",
      "reading word_len_list: \n",
      "(12,)\n",
      "reading input_len_list: \n",
      "(12,)\n",
      "('training data shapes:', (12, 11, 50, 100, 3), (12, 6))\n",
      "\n",
      "after dense1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 11, 50, 100,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "padding1 (ZeroPadding3D)        (None, 11, 54, 104,  0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 11, 27, 52, 3 2432        padding1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 11, 13, 26, 3 0           time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 11, 13, 26, 3 0           time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 11, 7, 13, 32 25632       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 11, 3, 6, 32) 0           time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 11, 3, 6, 32) 0           time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 11, 2, 3, 4)  3204        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 11, 1, 1, 4)  0           time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 11, 1, 1, 4)  0           time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 11, 4)        0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 11, 512)      400896      time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 11, 28)       14364       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 11, 28)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 446,528\n",
      "Trainable params: 446,528\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 7 samples, validate on 2 samples\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 2s 228ms/step - loss: 19.9443 - val_loss: 18.7787\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 19.8678 - val_loss: 18.7657\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 19.8949 - val_loss: 18.7488\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 19.8810 - val_loss: 18.7296\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 19.8369 - val_loss: 18.7111\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 19.7475 - val_loss: 18.6913\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 19.7346 - val_loss: 18.6681\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 19.7759 - val_loss: 18.6411\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 19.6813 - val_loss: 18.6066\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 19.6638 - val_loss: 18.5683\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 19.5640 - val_loss: 18.5189\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 19.4773 - val_loss: 18.4531\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 19.4150 - val_loss: 18.3700\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 19.5339 - val_loss: 18.2680\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 19.3493 - val_loss: 18.1467\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 19.1520 - val_loss: 18.0045\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 19.2652 - val_loss: 17.8431\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 19.1812 - val_loss: 17.6410\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 19.1233 - val_loss: 17.4264\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 18.8474 - val_loss: 17.2094\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 18.8224 - val_loss: 16.9764\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 18.6129 - val_loss: 16.7509\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 18.6509 - val_loss: 16.5344\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 18.4382 - val_loss: 16.3297\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 18.1705 - val_loss: 16.1457\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 18.4648 - val_loss: 15.9879\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 17.9167 - val_loss: 15.8522\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 17.9295 - val_loss: 15.7347\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 17.8806 - val_loss: 15.6375\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 17.5422 - val_loss: 15.5549\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 17.5409 - val_loss: 15.4859\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 17.5320 - val_loss: 15.4249\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 17.4263 - val_loss: 15.3753\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 17.2785 - val_loss: 15.3380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 17.1514 - val_loss: 15.3077\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 17.3307 - val_loss: 15.2823\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.9937 - val_loss: 15.2618\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 17.0381 - val_loss: 15.2447\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.9650 - val_loss: 15.2308\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 17.0313 - val_loss: 15.2194\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 17.0267 - val_loss: 15.2102\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.9326 - val_loss: 15.2027\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 17.1519 - val_loss: 15.1958\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.8646 - val_loss: 15.1899\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.8411 - val_loss: 15.1846\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.8192 - val_loss: 15.1798\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.8747 - val_loss: 15.1752\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 17.0197 - val_loss: 15.1710\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.7753 - val_loss: 15.1674\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.8518 - val_loss: 15.1641\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.7418 - val_loss: 15.1611\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.8797 - val_loss: 15.1584\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.7517 - val_loss: 15.1560\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.7289 - val_loss: 15.1540\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.7949 - val_loss: 15.1521\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.7245 - val_loss: 15.1504\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.7246 - val_loss: 15.1488\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6940 - val_loss: 15.1474\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.7261 - val_loss: 15.1460\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.7144 - val_loss: 15.1448\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.7655 - val_loss: 15.1437\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6718 - val_loss: 15.1427\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.7077 - val_loss: 15.1417\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6820 - val_loss: 15.1409\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6851 - val_loss: 15.1401\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6762 - val_loss: 15.1395\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6840 - val_loss: 15.1389\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.7272 - val_loss: 15.1383\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6515 - val_loss: 15.1377\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6513 - val_loss: 15.1372\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6679 - val_loss: 15.1366\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6590 - val_loss: 15.1362\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6876 - val_loss: 15.1357\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6860 - val_loss: 15.1353\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6585 - val_loss: 15.1349\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6925 - val_loss: 15.1345\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6651 - val_loss: 15.1341\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6708 - val_loss: 15.1337\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.7332 - val_loss: 15.1333\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6527 - val_loss: 15.1330\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6687 - val_loss: 15.1326\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6579 - val_loss: 15.1323\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6563 - val_loss: 15.1320\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6675 - val_loss: 15.1318\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6494 - val_loss: 15.1315\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6608 - val_loss: 15.1312\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6801 - val_loss: 15.1310\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6735 - val_loss: 15.1307\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6543 - val_loss: 15.1305\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6876 - val_loss: 15.1302\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6577 - val_loss: 15.1299\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6489 - val_loss: 15.1296\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6538 - val_loss: 15.1294\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6428 - val_loss: 15.1292\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6722 - val_loss: 15.1290\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6353 - val_loss: 15.1287\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6325 - val_loss: 15.1286\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6474 - val_loss: 15.1284\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 16.6495 - val_loss: 15.1282\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 16.6434 - val_loss: 15.1280\n",
      "Saving model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for speaker in speakers:\n",
    "    \n",
    "    print(\"hey let's loading the numpy data for speaker \" + speaker)\n",
    "    print(\"reading x_raw: \")\n",
    "    x_raw = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_x2.npy\")\n",
    "    x_raw = np.stack(x_raw, axis=0)\n",
    "#     x_raws.append(x_raw)\n",
    "    print(x_raw.shape)\n",
    "    print(\"reading y_raw: \")\n",
    "    y_raw = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_y2.npy\")\n",
    "#     y_raws.append(y_raw)\n",
    "    print(y_raw.shape)\n",
    "    print(\"reading word_len_list: \")\n",
    "    word_len_list = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_word_len_list2.npy\")\n",
    "#     np.append(word_len_lists, word_len_list)\n",
    "    print(word_len_list.shape)\n",
    "    print(\"reading input_len_list: \")\n",
    "    input_len_list = np.load(SAVE_NUMPY_PATH + \"/\" + speaker + \"_input_len_list2.npy\")\n",
    "#     np.append(input_len_lists, input_len_list)\n",
    "    print(input_len_list.shape)\n",
    "    \n",
    "    print(\"training data shapes:\", x_raw.shape, y_raw.shape)\n",
    "    x_train, x_test, y_train, y_test, label_len_train, label_len_test, \\\n",
    "    input_len_train, input_len_test = train_test_split(x_raw, y_raw, word_len_list, input_len_list, test_size=0.2)\n",
    "\n",
    "    model = build_model(x_raw.shape[1:], 28, max_string_len = 10)\n",
    "\n",
    "    history = train(model, x_train, y_train, label_len_train, input_len_train, epochs=epochs)\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    model.save('model.h5')\n",
    "\n",
    "    print(\"Done.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
