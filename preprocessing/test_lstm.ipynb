{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbifzp.mpg\n",
      "0: 16--21: bin\n",
      "1: 21--26: blue\n",
      "2: 26--28: in\n",
      "3: 28--32: f\n",
      "4: 32--40: zero\n",
      "5: 40--51: please\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbizzn.mpg\n",
      "6: 16--22: bin\n",
      "different size, skip\n",
      "6: 22--27: blue\n",
      "different size, skip\n",
      "6: 27--31: in\n",
      "different size, skip\n",
      "6: 31--36: z\n",
      "different size, skip\n",
      "6: 36--45: zero\n",
      "different size, skip\n",
      "6: 45--51: now\n",
      "different size, skip\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbal8p.mpg\n",
      "6: 22--27: bin\n",
      "7: 27--31: blue\n",
      "8: 31--32: at\n",
      "9: 32--36: l\n",
      "10: 36--40: eight\n",
      "11: 40--50: please\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbbf9a.mpg\n",
      "12: 22--27: bin\n",
      "13: 27--31: blue\n",
      "14: 31--35: by\n",
      "15: 35--39: f\n",
      "16: 39--45: nine\n",
      "17: 45--54: again\n",
      "0: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "1: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "2: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "3: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "4: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "5: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "6: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "7: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "8: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "9: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "10: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "11: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "12: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "13: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "14: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "15: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "16: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "17: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "('load data took', 25.85976004600525)\n",
      "('training data shapes:', (18, 11, 50, 100, 3), (18, 6))\n",
      "\n",
      "after dense1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 11, 50, 100,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "padding1 (ZeroPadding3D)        (None, 11, 54, 104,  0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 11, 27, 52, 3 2432        padding1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 11, 13, 26, 3 0           time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 11, 13, 26, 3 0           time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 11, 7, 13, 32 25632       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 11, 3, 6, 32) 0           time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 11, 3, 6, 32) 0           time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 11, 2, 3, 4)  3204        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 11, 1, 1, 4)  0           time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 11, 1, 1, 4)  0           time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 11, 4)        0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 11, 512)      400896      time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 11, 28)       14364       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 11, 28)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 446,528\n",
      "Trainable params: 446,528\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 11 samples, validate on 3 samples\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 2s 151ms/step - loss: 15.2048 - val_loss: 9.6699\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 15.1892 - val_loss: 9.6654\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 15.2186 - val_loss: 9.6594\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 15.2130 - val_loss: 9.6544\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 15.2130 - val_loss: 9.6492\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 15.1495 - val_loss: 9.6413\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 15.1678 - val_loss: 9.6329\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 15.1743 - val_loss: 9.6239\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 15.0949 - val_loss: 9.6148\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 15.1658 - val_loss: 9.6049\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 15.1138 - val_loss: 9.5959\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 15.0516 - val_loss: 9.5864\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 15.1077 - val_loss: 9.5766\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 15.0447 - val_loss: 9.5651\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 15.0708 - val_loss: 9.5532\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 15.0470 - val_loss: 9.5411\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.9725 - val_loss: 9.5294\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.8995 - val_loss: 9.5159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.9339 - val_loss: 9.5007\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.9661 - val_loss: 9.4845\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.9531 - val_loss: 9.4676\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.9021 - val_loss: 9.4502\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.8589 - val_loss: 9.4319\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.8814 - val_loss: 9.4122\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.8313 - val_loss: 9.3908\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.8337 - val_loss: 9.3698\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.7561 - val_loss: 9.3487\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.6977 - val_loss: 9.3267\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.6609 - val_loss: 9.3060\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.6145 - val_loss: 9.2743\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.5823 - val_loss: 9.2290\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.4922 - val_loss: 9.1724\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.4608 - val_loss: 9.1172\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.4839 - val_loss: 9.0547\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.5254 - val_loss: 8.9862\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.4059 - val_loss: 8.9162\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.3616 - val_loss: 8.8497\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.3383 - val_loss: 8.7954\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.3258 - val_loss: 8.7510\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.1721 - val_loss: 8.7164\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.2314 - val_loss: 8.6904\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.1775 - val_loss: 8.6723\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.0578 - val_loss: 8.6584\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.1825 - val_loss: 8.6455\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.0724 - val_loss: 8.6352\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.0178 - val_loss: 8.6264\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.0897 - val_loss: 8.6183\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.9986 - val_loss: 8.6126\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.9256 - val_loss: 8.6076\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.8596 - val_loss: 8.6034\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.8773 - val_loss: 8.5996\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.9428 - val_loss: 8.5964\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.8609 - val_loss: 8.5933\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.8094 - val_loss: 8.5908\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.8865 - val_loss: 8.5886\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.8618 - val_loss: 8.5866\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.8320 - val_loss: 8.5849\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.8188 - val_loss: 8.5834\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.8111 - val_loss: 8.5820\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.8375 - val_loss: 8.5808\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.7702 - val_loss: 8.5796\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.8040 - val_loss: 8.5786\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7710 - val_loss: 8.5776\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.8339 - val_loss: 8.5766\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7663 - val_loss: 8.5757\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.7666 - val_loss: 8.5749\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.8006 - val_loss: 8.5741\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7854 - val_loss: 8.5733\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.7727 - val_loss: 8.5726\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.8051 - val_loss: 8.5719\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7280 - val_loss: 8.5713\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7692 - val_loss: 8.5708\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7282 - val_loss: 8.5704\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7540 - val_loss: 8.5699\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7609 - val_loss: 8.5695\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7354 - val_loss: 8.5691\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7333 - val_loss: 8.5688\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7252 - val_loss: 8.5685\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7293 - val_loss: 8.5682\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7171 - val_loss: 8.5680\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7174 - val_loss: 8.5677\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7353 - val_loss: 8.5675\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7312 - val_loss: 8.5673\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.7188 - val_loss: 8.5671\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7368 - val_loss: 8.5669\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.6986 - val_loss: 8.5668\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7296 - val_loss: 8.5666\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7172 - val_loss: 8.5665\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7152 - val_loss: 8.5664\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.7449 - val_loss: 8.5662\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7156 - val_loss: 8.5660\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7008 - val_loss: 8.5659\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.7056 - val_loss: 8.5657\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7084 - val_loss: 8.5656\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7025 - val_loss: 8.5654\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7218 - val_loss: 8.5653\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.7115 - val_loss: 8.5651\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7700 - val_loss: 8.5650\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7117 - val_loss: 8.5648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7163 - val_loss: 8.5647\n",
      "Saving model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from data import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding3D\n",
    "from keras.layers.core import Lambda, Dropout, Flatten, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "DATA_PATH = CURRENT_PATH + '/../data'\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    import tensorflow as tf\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # From Keras example image_ocr.py:\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    # y_pred = y_pred[:, 2:, :]\n",
    "    label_length = K.cast(tf.squeeze(label_length),'int32')\n",
    "    input_length = K.cast(tf.squeeze(input_length),'int32')\n",
    "    labels = K.ctc_label_dense_to_sparse(labels, label_length)\n",
    "    #y_pred = y_pred[:, :, :]\n",
    "    #return K.ctc_batch_cost(labels, y_pred, input_length, label_length, ignore_longer_outputs_than_inputs=True)\n",
    "    return tf.nn.ctc_loss(labels, y_pred, input_length, ctc_merge_repeated=False,\n",
    "                         ignore_longer_outputs_than_inputs = True, time_major = False)\n",
    "def CTC(name, args):\n",
    "\treturn Lambda(ctc_lambda_func, output_shape=(1,), name=name)(args)\n",
    "\n",
    "\n",
    "def build_model(input_size, output_size = 28, max_string_len = 10):\n",
    "    # model = Sequential()\n",
    "    input_data = Input(name='the_input', shape=input_size, dtype='float32')\n",
    "    x = ZeroPadding3D(padding=(0,2,2), name='padding1')(input_data)\n",
    "    x = TimeDistributed(Conv2D(filters = 32, kernel_size = 5, strides = (2,2),\n",
    "                             padding = 'same', activation = 'relu'))(x)\n",
    "    print\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(filters=32, kernel_size=5, strides=(2, 2),\n",
    "                               padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(filters=4, kernel_size=5, strides=(2, 2),\n",
    "                               padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    input_lstm = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    x_lstm = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(input_lstm)\n",
    "    x_lstm = Dense(output_size, kernel_initializer='he_normal', name='dense1')(x_lstm)\n",
    "    print(\"after dense1\")\n",
    "    y_pred = Activation('softmax', name='softmax')(x_lstm)\n",
    "\n",
    "    labels = Input(name='the_labels', shape = [max_string_len], dtype='int32')\n",
    "    input_length = Input(name = 'input_length', shape =[1], dtype = 'int32')\n",
    "    label_length = Input(name = 'label_length', shape = [1], dtype = 'int32')\n",
    "    loss = CTC('ctc',[y_pred, labels, input_length, label_length])\n",
    "    model = Model(inputs=[input_data, labels, label_length, input_length],\n",
    "                  outputs = loss)\n",
    "    model.summary()\n",
    "    # Build model here...\n",
    "\n",
    "    return model\n",
    "def pad_labels(labels, max_string_len):\n",
    "    padding = np.ones((labels.shape[0], max_string_len - labels.shape[1])) * -1\n",
    "    return np.concatenate((labels, padding), axis = 1)\n",
    "\n",
    "def train(model, x_train, y_train, label_len_train, input_len_train, batch_size=256, epochs=100, val_train_ratio=0.2):\n",
    "    max_string_len = 10\n",
    "    if y_train.shape[1] != max_string_len:\n",
    "        y_train = pad_labels(y_train, max_string_len)\n",
    "\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)\n",
    "    history = model.fit(x = {'the_input':x_train, 'the_labels':y_train, 'label_length':label_len_train,\n",
    "                             'input_length':input_len_train}, y = {'ctc': np.zeros([x_train.shape[0]])},\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=val_train_ratio,\n",
    "                        shuffle=True,\n",
    "                        verbose=1)\n",
    "\n",
    "    return history\n",
    "\n",
    "def read_data():\n",
    "    oh = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    x = list()\n",
    "    y = list()\n",
    "    t = list()\n",
    "    print(\"loading images...\")\n",
    "    for i, (img, words) in enumerate(load_data(DATA_PATH, verbose=False, framebyframe=False)):\n",
    "        if img.shape[0] != 75:\n",
    "            continue\n",
    "        x.append(img)\n",
    "        y.append(words)\n",
    "\n",
    "        t += words.tolist()\n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "    t = le.fit_transform(t)\n",
    "    oh.fit(t.reshape(-1, 1))\n",
    "\n",
    "    print(\"convering to np array...\")\n",
    "    x = np.stack(x, axis=0)\n",
    "\n",
    "    print(\"transforming y...\")\n",
    "    for i in range(len(y)):\n",
    "        y_ = le.transform(y[i])\n",
    "        y[i] = np.asarray(oh.transform(y_.reshape(-1, 1)).todense())\n",
    "    y = np.stack(y, axis=0)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def main():\n",
    "    epochs = 100\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"loading data\")\n",
    "    x, y, label_len, input_len = load_data(DATA_PATH, verbose=True, num_samples=18, ctc_encoding=True)\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"load data took\", end-start)\n",
    "    print(\"training data shapes:\", x.shape, y.shape)\n",
    "    x_train, x_test, y_train, y_test, label_len_train, label_len_test, \\\n",
    "    input_len_train, input_len_test = train_test_split(x, y, label_len, input_len, test_size=0.2)\n",
    "\n",
    "    model = build_model(x.shape[1:], 28, max_string_len = 10)\n",
    "\n",
    "    history = train(model, x_train, y_train, label_len_train, input_len_train, epochs=epochs)\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    model.save('model.h5')\n",
    "\n",
    "    # TODO: add visualization\n",
    "#     print(\"Plotting...\")\n",
    "    #f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "    #ax1.plot(range(1, epochs+1), history.history['val_acc'], 'tab:blue', label=\"validation accuracy\")\n",
    "    #ax1.plot(range(1, epochs+1), history.history['acc'], 'tab:red', label=\"training accuracy\")\n",
    "\n",
    "    #ax2.plot(range(1, epochs+1), history.history['loss'], 'tab:orange', label=\"loss\")\n",
    "    #ax2.plot(range(1, epochs+1), history.history['val_loss'], 'tab:green', label=\"validation loss\")\n",
    "\n",
    "    #ax1.legend()\n",
    "    #ax2.legend()\n",
    "\n",
    "    #f.savefig('training.png', dpi=300)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('new speaker added: ', 's13')\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae2p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq9s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaezn.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak4n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae1s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq8n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak7a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak5s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae3a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak6p.mpg\n",
      "('new speaker added: ', 's1')\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbifzp.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbizzn.mpg\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbal8p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbbf9a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbas2p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbaf5a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbir6n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbil5a.mpg\n"
     ]
    }
   ],
   "source": [
    "from data import load_data\n",
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "DATA_PATH = CURRENT_PATH + '/../data'\n",
    "x, y, label_len, input_len = load_data(DATA_PATH, verbose=True, num_samples=100, ctc_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new speaker added: s13\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae2p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq9s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaezn.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak4n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae1s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbaq8n.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak7a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak5s.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbae3a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s13/video/bbak6p.mpg\n",
      "new speaker added: s1\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbifzp.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbizzn.mpg\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "different size, skip\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbal8p.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbbf9a.mpg\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbas2p.mpg\n"
     ]
    }
   ],
   "source": [
    "from data import load_data\n",
    "import numpy as np\n",
    "\n",
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "DATA_PATH = CURRENT_PATH + '/../data'\n",
    "SAVE_NUMPY_PATH = CURRENT_PATH + '/../data/numpy_results/'\n",
    "\n",
    "speakers = load_data(DATA_PATH, verbose=True, num_samples=80, ctc_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey let's read the numpy results for speaker s13\n",
      "reading x_raw: \n",
      "(60,)\n",
      "reading y_raw: \n",
      "(60,)\n",
      "reading word_len_list: \n",
      "(60,)\n",
      "reading input_len_list: \n",
      "(60,)\n",
      "hey let's read the numpy results for speaker s1\n",
      "reading x_raw: \n",
      "(20,)\n",
      "reading y_raw: \n",
      "(20,)\n",
      "reading word_len_list: \n",
      "(20,)\n",
      "reading input_len_list: \n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "for speaker in speakers:\n",
    "    print(\"hey let's read the numpy results for speaker \" + speaker)\n",
    "    print(\"reading x_raw: \")\n",
    "    x_raw = np.load(SAVE_NUMPY_PATH + speaker + \"_x.npy\")\n",
    "    print(x_raw.shape)\n",
    "    print(\"reading y_raw: \")\n",
    "    y_raw = np.load(SAVE_NUMPY_PATH + speaker + \"_y.npy\")\n",
    "    print(y_raw.shape)\n",
    "    print(\"reading word_len_list: \")\n",
    "    word_len_list = np.load(SAVE_NUMPY_PATH + speaker + \"_word_len_list.npy\")\n",
    "    print(word_len_list.shape)\n",
    "    print(\"reading input_len_list: \")\n",
    "    input_len_list = np.load(SAVE_NUMPY_PATH + speaker + \"_input_len_list.npy\")\n",
    "    print(input_len_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from align import read_align\n",
    "from video import read_video\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import re\n",
    "\n",
    "CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "DATA_PATH = CURRENT_PATH + '/../data'\n",
    "PREDICTOR_PATH = CURRENT_PATH + '/shape_predictor_68_face_landmarks.dat'\n",
    "SAVE_NUMPY_PATH = CURRENT_PATH + '/../data/numpy_results'\n",
    "\n",
    "\n",
    "def text_to_labels(text):\n",
    "    ret = []\n",
    "    for char in text:\n",
    "        if char >= 'a' and char <= 'z':\n",
    "            ret.append(ord(char) - ord('a'))\n",
    "        elif char == ' ':\n",
    "            ret.append(26)\n",
    "    return ret\n",
    "\n",
    "def labels_to_text(labels):\n",
    "# 26 is space, 27 is CTC blank char\n",
    "    text = ''\n",
    "    for c in labels:\n",
    "        if c >= 0 and c < 26:\n",
    "            text += chr(c + ord('a'))\n",
    "        elif c == 26:\n",
    "            text += ' '\n",
    "    return text\n",
    "\n",
    "def load_data(datapath, verbose=False, num_samples=-1, ctc_encoding=False):\n",
    "    oh = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    counter = 0\n",
    "    done = False\n",
    "\n",
    "    max_len = 0\n",
    "    max_word_len = 0\n",
    "\n",
    "    word_len_list = []\n",
    "    input_len_list = []\n",
    "\n",
    "    x_raw = list()\n",
    "    y_raw = list()\n",
    "    \n",
    "    pattern = re.compile(\"s[0-9]\") \n",
    "    speakers = []\n",
    "    x_raws = {}\n",
    "    y_raws = {}\n",
    "    word_len_lists = {}\n",
    "    input_len_lists = {}\n",
    "    prev_speaker_count = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(datapath):\n",
    "        check = root.split(\"/\")[-1]\n",
    "        match = pattern.findall(check)\n",
    "        if (len(match) > 0):\n",
    "            if check.index(match[0]) == 0 and check not in speakers:\n",
    "#                 if len(speakers) > 0:\n",
    "#                     x_raws.append(x_raw[prev_speaker_count : counter])\n",
    "#                     prev_speaker_count = counter\n",
    "#                     if verbose:\n",
    "#                         print(\"previous speaker x_raws\")\n",
    "#                         print(x_raws)               \n",
    "                speakers.append(check)\n",
    "                x_raws[check] = []\n",
    "                y_raws[check] = []\n",
    "                if verbose:\n",
    "                    print(\"new speaker added\")\n",
    "                    print(check, match[0])             \n",
    "        \n",
    "        for name in files:\n",
    "            if '.mpg' in name:\n",
    "                if verbose is True:\n",
    "                    print(\"reading: \" + root + \"/\" + name)\n",
    "\n",
    "                video = read_video(os.path.join(root, name), PREDICTOR_PATH)\n",
    "                alignments = read_align(os.path.join(root, '../align/', name.split(\".\")[0] + \".align\"))\n",
    "                \n",
    "                for start, stop, word in alignments:\n",
    "                    if word == 'sil' or word == 'sp':\n",
    "                        continue\n",
    "                    \n",
    "                    if verbose is True:\n",
    "                        print(str(counter) + \": \" + str(start) + \"--\" + str(stop) + \": \" + word)\n",
    "                    \n",
    "                    _, d1, d2, d3 = video[start:stop].shape\n",
    "                    \n",
    "                    if (len(x_raw) > 0):\n",
    "                        _, prev_d1, prev_d2, prev_d3 = x_raw[-1].shape\n",
    "                        if (d1, d2, d3) != (prev_d1, prev_d2, prev_d3):\n",
    "                            if verbose is True:\n",
    "                                print(\"different size, skip\")\n",
    "                            continue\n",
    "                    \n",
    "                    x_raw.append(video[start:stop])\n",
    "                    y_raw.append(word)\n",
    "                    temp = video[start:stop]\n",
    "                    x_raws[speakers[-1]].append(temp)\n",
    "                    y_raws[speakers[-1]].append(word)\n",
    "                    \n",
    "                    max_word_len = max(max_word_len, len(word))\n",
    "                    max_len = max(max_len, stop-start)\n",
    "                    word_len_list.append(len(word))\n",
    "                    input_len_list.append(stop-start)\n",
    "                    word_len_lists[speakers[-1]].append(len(word))\n",
    "                    input_len_lists[speakers[-1]].append(stop-start)\n",
    "                    counter += 1\n",
    "                    if counter == num_samples:\n",
    "                        done = True\n",
    "                        break\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if not ctc_encoding:\n",
    "        y_raw = le.fit_transform(y_raw)\n",
    "#         y = oh.fit_transform(y_raw.reshape(-1, 1)).todense()\n",
    "\n",
    "\n",
    "    for i in range(len(x_raw)):\n",
    "        result = np.zeros((max_len, x_raw[i].shape[1], x_raw[i].shape[2], x_raw[i].shape[3]))\n",
    "        result[:x_raw[i].shape[0], :x_raw[i].shape[1], :x_raw[i].shape[2], :x_raw[i].shape[3]] = x_raw[i]\n",
    "        if verbose is True:\n",
    "            print(str(i) + \": \" + str(result.shape))\n",
    "        x_raw[i] = result\n",
    "\n",
    "        if verbose is True:\n",
    "            print(\"Added: \" + str(x_raw[i].shape)) \n",
    "        \n",
    "        if ctc_encoding:\n",
    "            res = np.ones(max_word_len) * -1\n",
    "            enc = np.array(text_to_labels(y_raw[i]))\n",
    "            res[:enc.shape[0]] = enc\n",
    "            y_raw[i] = res\n",
    "\n",
    "    for speaker in speakers:\n",
    "        np_save = SAVE_NUMPY_PATH + \"/\" + speaker\n",
    "        np.save(np_save + \"_x\", x_raws[speaker])\n",
    "        np.save(np_save + \"_y\", y_raws[speaker])\n",
    "        np.save(np_save + \"_word_len_list\", np.array(word_len_lists[speaker]))\n",
    "        np.save(np_save + \"_input_len_list\", np.array(input_len_lists[speaker]))\n",
    "        \n",
    "#     if ctc_encoding:\n",
    "#         y = np.stack(y_raw, axis=0)\n",
    "\n",
    "#     x = np.stack(x_raw, axis=0)\n",
    "\n",
    "#     print(root)\n",
    "#     np.save(\"test_savex\", x_raw)\n",
    "#     np.save(\"test_savey\", y_raw)\n",
    "#     return x, y, np.array(word_len_list), np.array(input_len_list)\n",
    "    return speakers #, np.array(word_len_list), np.array(input_len_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X, y = load_data(DATA_PATH, verbose=True, ctc_encoding=True, num_samples=15)\n",
    "    print(\"X:\", X.shape)\n",
    "    print(\"y:\", y.shape)\n",
    "    print(y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
