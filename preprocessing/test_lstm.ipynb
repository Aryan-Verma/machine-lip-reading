{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbifzp.mpg\n",
      "0: 16--21: bin\n",
      "(50, 100, 3)\n",
      "1: 21--26: blue\n",
      "(50, 100, 3)\n",
      "2: 26--28: in\n",
      "(50, 100, 3)\n",
      "3: 28--32: f\n",
      "(50, 100, 3)\n",
      "4: 32--40: zero\n",
      "(50, 100, 3)\n",
      "5: 40--51: please\n",
      "(50, 100, 3)\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbizzn.mpg\n",
      "6: 16--22: bin\n",
      "(288, 360, 3)\n",
      "different size, skip\n",
      "6: 22--27: blue\n",
      "(288, 360, 3)\n",
      "different size, skip\n",
      "6: 27--31: in\n",
      "(288, 360, 3)\n",
      "different size, skip\n",
      "6: 31--36: z\n",
      "(288, 360, 3)\n",
      "different size, skip\n",
      "6: 36--45: zero\n",
      "(288, 360, 3)\n",
      "different size, skip\n",
      "6: 45--51: now\n",
      "(288, 360, 3)\n",
      "different size, skip\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbal8p.mpg\n",
      "6: 22--27: bin\n",
      "(50, 100, 3)\n",
      "7: 27--31: blue\n",
      "(50, 100, 3)\n",
      "8: 31--32: at\n",
      "(50, 100, 3)\n",
      "9: 32--36: l\n",
      "(50, 100, 3)\n",
      "10: 36--40: eight\n",
      "(50, 100, 3)\n",
      "11: 40--50: please\n",
      "(50, 100, 3)\n",
      "reading: /home/ubuntu/assignments/machine-lip-reading/preprocessing/../data/s1/video/bbbf9a.mpg\n",
      "12: 22--27: bin\n",
      "(50, 100, 3)\n",
      "13: 27--31: blue\n",
      "(50, 100, 3)\n",
      "14: 31--35: by\n",
      "(50, 100, 3)\n",
      "15: 35--39: f\n",
      "(50, 100, 3)\n",
      "16: 39--45: nine\n",
      "(50, 100, 3)\n",
      "17: 45--54: again\n",
      "(50, 100, 3)\n",
      "0: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "1: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "2: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "3: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "4: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "5: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "6: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "7: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "8: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "9: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "10: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "11: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "12: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "13: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "14: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "15: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "16: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "17: (11, 50, 100, 3)\n",
      "Added: (11, 50, 100, 3)\n",
      "(6,)\n",
      "('load data took', 25.753634929656982)\n",
      "('training data shapes:', (18, 11, 50, 100, 3), (18, 6))\n",
      "\n",
      "after dense1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 11, 50, 100,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "padding1 (ZeroPadding3D)        (None, 11, 54, 104,  0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 11, 27, 52, 3 2432        padding1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 11, 13, 26, 3 0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 11, 13, 26, 3 0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 11, 7, 13, 32 25632       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 11, 3, 6, 32) 0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 11, 3, 6, 32) 0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 11, 2, 3, 4)  3204        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 11, 1, 1, 4)  0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 11, 1, 1, 4)  0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 11, 4)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 11, 512)      400896      time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 11, 28)       14364       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 11, 28)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 446,528\n",
      "Trainable params: 446,528\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 11 samples, validate on 3 samples\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 2s 137ms/step - loss: 14.8491 - val_loss: 12.7353\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.8455 - val_loss: 12.7055\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.7937 - val_loss: 12.6692\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.8242 - val_loss: 12.6302\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.8014 - val_loss: 12.5885\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.7407 - val_loss: 12.5454\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.7179 - val_loss: 12.4980\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.6826 - val_loss: 12.4466\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.6377 - val_loss: 12.3898\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.6229 - val_loss: 12.3330\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.6767 - val_loss: 12.2817\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.5371 - val_loss: 12.2315\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.4495 - val_loss: 12.1783\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.5589 - val_loss: 12.1302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.3914 - val_loss: 12.0759\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.3874 - val_loss: 12.0244\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.2375 - val_loss: 11.9706\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.2703 - val_loss: 11.9112\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.2233 - val_loss: 11.8422\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.2344 - val_loss: 11.7680\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 14.1662 - val_loss: 11.6962\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 14.0602 - val_loss: 11.6306\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.9056 - val_loss: 11.5657\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.7280 - val_loss: 11.5023\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7632 - val_loss: 11.4392\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.6716 - val_loss: 11.3757\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7764 - val_loss: 11.3141\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.6856 - val_loss: 11.2520\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.5974 - val_loss: 11.1917\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.7462 - val_loss: 11.1331\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.6428 - val_loss: 11.0775\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.5929 - val_loss: 11.0234\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.4472 - val_loss: 10.9706\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.5703 - val_loss: 10.9215\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.3759 - val_loss: 10.8752\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.4058 - val_loss: 10.8336\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.3698 - val_loss: 10.7955\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.3783 - val_loss: 10.7617\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.3590 - val_loss: 10.7295\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.2420 - val_loss: 10.7007\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.2774 - val_loss: 10.6739\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.2991 - val_loss: 10.6498\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.2398 - val_loss: 10.6285\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.1448 - val_loss: 10.6092\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.1687 - val_loss: 10.5916\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.1812 - val_loss: 10.5756\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.1568 - val_loss: 10.5614\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.2070 - val_loss: 10.5482\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.1902 - val_loss: 10.5362\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.1740 - val_loss: 10.5251\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0968 - val_loss: 10.5150\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.1883 - val_loss: 10.5056\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0873 - val_loss: 10.4971\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.1276 - val_loss: 10.4896\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0921 - val_loss: 10.4826\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0862 - val_loss: 10.4762\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0922 - val_loss: 10.4703\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0619 - val_loss: 10.4648\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.1096 - val_loss: 10.4597\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0919 - val_loss: 10.4549\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0957 - val_loss: 10.4502\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0733 - val_loss: 10.4460\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0651 - val_loss: 10.4421\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0696 - val_loss: 10.4384\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0997 - val_loss: 10.4349\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0764 - val_loss: 10.4316\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0801 - val_loss: 10.4285\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0715 - val_loss: 10.4257\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0605 - val_loss: 10.4230\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0640 - val_loss: 10.4205\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0617 - val_loss: 10.4181\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0566 - val_loss: 10.4158\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0507 - val_loss: 10.4136\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0725 - val_loss: 10.4117\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0511 - val_loss: 10.4099\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0673 - val_loss: 10.4081\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0989 - val_loss: 10.4064\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0678 - val_loss: 10.4049\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0792 - val_loss: 10.4034\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0622 - val_loss: 10.4019\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0314 - val_loss: 10.4005\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0323 - val_loss: 10.3992\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0525 - val_loss: 10.3979\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0489 - val_loss: 10.3968\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0746 - val_loss: 10.3957\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0452 - val_loss: 10.3946\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0420 - val_loss: 10.3937\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0425 - val_loss: 10.3927\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0778 - val_loss: 10.3918\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0330 - val_loss: 10.3910\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0593 - val_loss: 10.3900\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0398 - val_loss: 10.3892\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.1142 - val_loss: 10.3883\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0474 - val_loss: 10.3875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0361 - val_loss: 10.3866\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0419 - val_loss: 10.3859\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0858 - val_loss: 10.3850\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0484 - val_loss: 10.3842\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 13.0360 - val_loss: 10.3835\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 13.0410 - val_loss: 10.3828\n",
      "Saving model...\n",
      "Plotting...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from data import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding3D\n",
    "from keras.layers.core import Lambda, Dropout, Flatten, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "CURRENT_PATH = '/home/ubuntu/assignments/machine-lip-reading/preprocessing'\n",
    "DATA_PATH = CURRENT_PATH + '/../data'\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    import tensorflow as tf\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # From Keras example image_ocr.py:\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    # y_pred = y_pred[:, 2:, :]\n",
    "    label_length = K.cast(tf.squeeze(label_length),'int32')\n",
    "    input_length = K.cast(tf.squeeze(input_length),'int32')\n",
    "    labels = K.ctc_label_dense_to_sparse(labels, label_length)\n",
    "    #y_pred = y_pred[:, :, :]\n",
    "    #return K.ctc_batch_cost(labels, y_pred, input_length, label_length, ignore_longer_outputs_than_inputs=True)\n",
    "    return tf.nn.ctc_loss(labels, y_pred, input_length, ctc_merge_repeated=False,\n",
    "                         ignore_longer_outputs_than_inputs = True, time_major = False)\n",
    "def CTC(name, args):\n",
    "\treturn Lambda(ctc_lambda_func, output_shape=(1,), name=name)(args)\n",
    "\n",
    "\n",
    "def build_model(input_size, output_size = 28, max_string_len = 10):\n",
    "    # model = Sequential()\n",
    "    input_data = Input(name='the_input', shape=input_size, dtype='float32')\n",
    "    x = ZeroPadding3D(padding=(0,2,2), name='padding1')(input_data)\n",
    "    x = TimeDistributed(Conv2D(filters = 32, kernel_size = 5, strides = (2,2),\n",
    "                             padding = 'same', activation = 'relu'))(x)\n",
    "    print\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(filters=32, kernel_size=5, strides=(2, 2),\n",
    "                               padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = TimeDistributed(Conv2D(filters=4, kernel_size=5, strides=(2, 2),\n",
    "                               padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=None, name='max1'))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    input_lstm = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    x_lstm = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(input_lstm)\n",
    "    x_lstm = Dense(output_size, kernel_initializer='he_normal', name='dense1')(x_lstm)\n",
    "    print(\"after dense1\")\n",
    "    y_pred = Activation('softmax', name='softmax')(x_lstm)\n",
    "\n",
    "    labels = Input(name='the_labels', shape = [max_string_len], dtype='int32')\n",
    "    input_length = Input(name = 'input_length', shape =[1], dtype = 'int32')\n",
    "    label_length = Input(name = 'label_length', shape = [1], dtype = 'int32')\n",
    "    loss = CTC('ctc',[y_pred, labels, input_length, label_length])\n",
    "    model = Model(inputs=[input_data, labels, label_length, input_length],\n",
    "                  outputs = loss)\n",
    "    model.summary()\n",
    "    # Build model here...\n",
    "\n",
    "    return model\n",
    "def pad_labels(labels, max_string_len):\n",
    "    padding = np.ones((labels.shape[0], max_string_len - labels.shape[1])) * -1\n",
    "    return np.concatenate((labels, padding), axis = 1)\n",
    "\n",
    "def train(model, x_train, y_train, label_len_train, input_len_train, batch_size=256, epochs=100, val_train_ratio=0.2):\n",
    "    max_string_len = 10\n",
    "    if y_train.shape[1] != max_string_len:\n",
    "        y_train = pad_labels(y_train, max_string_len)\n",
    "\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)\n",
    "    history = model.fit(x = {'the_input':x_train, 'the_labels':y_train, 'label_length':label_len_train,\n",
    "                             'input_length':input_len_train}, y = {'ctc': np.zeros([x_train.shape[0]])},\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=val_train_ratio,\n",
    "                        shuffle=True,\n",
    "                        verbose=1)\n",
    "\n",
    "    return history\n",
    "\n",
    "def read_data():\n",
    "    oh = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    x = list()\n",
    "    y = list()\n",
    "    t = list()\n",
    "    print(\"loading images...\")\n",
    "    for i, (img, words) in enumerate(load_data(DATA_PATH, verbose=False, framebyframe=False)):\n",
    "        if img.shape[0] != 75:\n",
    "            continue\n",
    "        x.append(img)\n",
    "        y.append(words)\n",
    "\n",
    "        t += words.tolist()\n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "    t = le.fit_transform(t)\n",
    "    oh.fit(t.reshape(-1, 1))\n",
    "\n",
    "    print(\"convering to np array...\")\n",
    "    x = np.stack(x, axis=0)\n",
    "\n",
    "    print(\"transforming y...\")\n",
    "    for i in range(len(y)):\n",
    "        y_ = le.transform(y[i])\n",
    "        y[i] = np.asarray(oh.transform(y_.reshape(-1, 1)).todense())\n",
    "    y = np.stack(y, axis=0)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def main():\n",
    "    epochs = 100\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"loading data\")\n",
    "    x, y, label_len, input_len= load_data(DATA_PATH, verbose=True, num_samples=18, ctc_encoding=True)\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"load data took\", end-start)\n",
    "    print(\"training data shapes:\", x.shape, y.shape)\n",
    "    x_train, x_test, y_train, y_test, label_len_train, label_len_test, \\\n",
    "    input_len_train, input_len_test = train_test_split(x, y, label_len, input_len, test_size=0.2)\n",
    "\n",
    "    model = build_model(x.shape[1:], 28, max_string_len = 10)\n",
    "\n",
    "    history = train(model, x_train, y_train, label_len_train, input_len_train, epochs=epochs)\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    model.save('model.h5')\n",
    "\n",
    "    # TODO: add visualization\n",
    "    print(\"Plotting...\")\n",
    "    #f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "    #ax1.plot(range(1, epochs+1), history.history['val_acc'], 'tab:blue', label=\"validation accuracy\")\n",
    "    #ax1.plot(range(1, epochs+1), history.history['acc'], 'tab:red', label=\"training accuracy\")\n",
    "\n",
    "    #ax2.plot(range(1, epochs+1), history.history['loss'], 'tab:orange', label=\"loss\")\n",
    "    #ax2.plot(range(1, epochs+1), history.history['val_loss'], 'tab:green', label=\"validation loss\")\n",
    "\n",
    "    #ax1.legend()\n",
    "    #ax2.legend()\n",
    "\n",
    "    #f.savefig('training.png', dpi=300)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
